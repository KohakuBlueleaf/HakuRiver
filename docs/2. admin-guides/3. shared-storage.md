# Shared Storage Guide

Shared storage is a crucial, **mandatory** component of the HakuRiver system. It enables container tarballs to be distributed to Runners and provides a common location for task logs and data. This guide explains how to set up and configure shared storage for your HakuRiver cluster.

## Key Concept: Physical vs. Logical Storage

An important concept to understand in HakuRiver's shared storage architecture:

-   **Physical Storage**: The actual storage medium where data is stored (NFS server, Samba share, GlusterFS volume, etc.).
-   **Logical Path**: The filesystem path where the storage is mounted on each node.

**Important**: The physical storage must be the same across all nodes, but the logical mount paths can differ on different nodes, as long as the `shared_dir` configuration setting on each node reflects its *local* mount path for that shared physical storage.

Example configuration:
-   On the Host: `shared_dir = "/mnt/hakuriver-shared"`
-   On Runner 1: `shared_dir = "/mnt/shared"`
-   On Runner 2: `shared_dir = "/shared_storage"`

As long as `/mnt/hakuriver-shared` on the Host, `/mnt/shared` on Runner 1, and `/shared_storage` on Runner 2 all point to the *same physical filesystem location*, HakuRiver will function properly.

## Shared Storage Requirements

HakuRiver needs shared storage for:

1.  **Container Tarballs**: Docker images packaged for distribution to Runners (`shared_dir/container_dir/`). Created by the Host, read by Runners.
2.  **Task Output Logs**: Standard output and error from Command tasks (`shared_dir/task_outputs/`, `shared_dir/task_errors/`). Written by Runners, read by the Host (for logs API/Web UI).
3.  **Shared Data**: Input/output data, scripts, etc., for tasks (`shared_dir/shared_data/` is conventionally used and mounted into Docker tasks as `/shared`). Read and written by tasks on Runners.

### Minimum Specifications

-   **Size**: 20GB+ (dependent on the size and number of container images and the amount of task data/logs). Plan for growth.
-   **Performance**:
    -   Moderate read speed for container distribution (can be large files).
    -   Moderate write speed for task logs (potentially concurrent writes from many tasks/runners).
    -   Reasonable latency for file access by tasks.
-   **Access**: Read/write permission for the user(s) running HakuRiver Host and Runner components on all nodes.
-   **Reliability**: Sufficient for your production needs.

## Storage Options

You can use various network filesystems. Choose one that is suitable for your operating systems and network environment.

### Network File System (NFS)

NFS is a common choice for Linux-only or mixed Linux/Unix environments.

**Server Setup (Example for Ubuntu/Debian)**:
```bash
# Install NFS server package
sudo apt update
sudo apt install nfs-kernel-server

# Create the directory to be exported
sudo mkdir -p /export/hakuriver

# Set appropriate permissions (adjust owner/group based on your Runner user)
# A common simple, but less secure, approach is allowing world write for testing:
sudo chown nobody:nogroup /export/hakuriver # Or a specific user/group
sudo chmod 777 /export/hakuriver # Adjust permissions carefully for production

# Configure exports: Add a line to /etc/exports
# Example allows access from any host (*) with read/write, sync writes, etc.
echo '/export/hakuriver *(rw,sync,no_subtree_check,no_root_squash)' | sudo tee -a /etc/exports
# For specific client IPs/networks: '/export/hakuriver 192.168.1.0/24(rw,sync,no_subtree_check)'

# Apply the new exports
sudo exportfs -a

# Restart the NFS server service
sudo systemctl restart nfs-kernel-server
```

**Client Setup (On Host and all Runner nodes)**:
```bash
# Install NFS client package
sudo apt update
sudo apt install nfs-common

# Create the local mount point directory
sudo mkdir -p /mnt/hakuriver-shared # This is the 'logical path' for THIS node

# Mount the NFS share
# Replace 'nfs-server' with the hostname or IP of your NFS server
sudo mount -t nfs nfs-server:/export/hakuriver /mnt/hakuriver-shared

# Add to /etc/fstab for persistence across reboots
# Replace 'nfs-server' and '/mnt/hakuriver-shared' with your details
echo 'nfs-server:/export/hakuriver /mnt/hakuriver-shared nfs defaults 0 0' | sudo tee -a /etc/fstab

# Verify the mount:
df -h /mnt/hakuriver-shared
```

### Samba/CIFS

Samba is useful for environments including Windows servers or desktops acting as file servers.

**Server Setup (On Windows or Linux Samba server)**:
On Linux:
```bash
# Install Samba package
sudo apt install samba

# Create the directory to be shared
sudo mkdir -p /export/hakuriver # Or use an existing directory

# Configure the share by editing the Samba configuration file
# sudo vim /etc/samba/smb.conf
# Add a section like this:
# [hakuriver]
#    path = /export/hakuriver
#    browseable = yes
#    read only = no
#    guest ok = yes # Allows access without a Samba user (simple, less secure)
#    # OR require a user: valid users = your_samba_user
#    create mask = 0777
#    directory mask = 0777

# Restart the Samba service
sudo systemctl restart smbd
```

**Client Setup (On Host and all Runner nodes)**:
```bash
# Install CIFS utils package
sudo apt install cifs-utils

# Create the local mount point directory
sudo mkdir -p /mnt/hakuriver-shared # This is the 'logical path' for THIS node

# Mount the CIFS share (using guest access or credentials)
# Replace 'samba-server' and 'sharename' with your details
# Guest access example (if server allows it):
# sudo mount -t cifs //samba-server/hakuriver /mnt/hakuriver-shared -o guest,iocharset=utf8
# With credentials example:
# sudo mount -t cifs //samba-server/hakuriver /mnt/hakuriver-shared -o username=your_samba_user,password=your_password,iocharset=utf8

# For automated mounting in /etc/fstab, it's best to use a separate credentials file
# Create /etc/hakuriver-credentials with content:
# username=your_samba_user
# password=your_password
# domain=your_domain # Optional
# sudo chmod 600 /etc/hakuriver-credentials # Protect credentials file

# Add to /etc/fstab for persistence (example with credentials file)
# //samba-server/hakuriver /mnt/hakuriver-shared cifs credentials=/etc/hakuriver-credentials,iocharset=utf8 0 0

# Verify the mount:
df -h /mnt/hakuriver-shared
```

### GlusterFS

GlusterFS provides a scalable, distributed filesystem across multiple servers.

**Server Setup (On servers participating in the Gluster cluster)**:
```bash
# Install GlusterFS server on all nodes in the storage cluster
sudo apt install glusterfs-server # Or use appropriate package manager

# Start and enable glusterd service
sudo systemctl start glusterd
sudo systemctl enable glusterd

# On the first Gluster node, create a trusted storage pool (probe other nodes)
# Replace 'storage-node2' and 'storage-node3' with actual hostnames/IPs
sudo gluster peer probe storage-node2
# Repeat for all nodes in the Gluster pool

# Create a directory on each node that will be used as a 'brick'
# This directory should be on a dedicated partition/disk typically
sudo mkdir -p /gluster/brick1/hakuriver

# On the first Gluster node, create a volume (example: replicated volume)
# Replace hostnames and brick paths
sudo gluster volume create hakuriver-vol replica 3 \
  storage-node1:/gluster/brick1/hakuriver \
  storage-node2:/gluster/brick1/hakuriver \
  storage-node3:/gluster/brick1/hakuriver force

# Start the volume
sudo gluster volume start hakuriver-vol
```

**Client Setup (On Host and all Runner nodes)**:
```bash
# Install GlusterFS client package
sudo apt install glusterfs-client # Or use appropriate package manager

# Create the local mount point directory
sudo mkdir -p /mnt/hakuriver-shared # This is the 'logical path' for THIS node

# Mount the GlusterFS volume
# Replace 'storage-node1' with one of the nodes in the Gluster pool
sudo mount -t glusterfs storage-node1:/hakuriver-vol /mnt/hakuriver-shared

# Add to /etc/fstab for persistence
# _netdev option ensures mount happens after network is up
echo 'storage-node1:/hakuriver-vol /mnt/hakuriver-shared glusterfs defaults,_netdev 0 0' | sudo tee -a /etc/fstab

# Verify the mount:
df -h /mnt/hakuriver-shared
```

### Other Options

-   **Ceph**: A highly scalable, software-defined storage system offering object, block, and file storage.
-   **MooseFS**: A fault-tolerant, network distributed file system.
-   **Cloud-based Shared Storage**: For deployments in cloud environments (e.g., Amazon EFS, Azure Files, Google Cloud Filestore).
-   **Local shared disk**: Possible for single-machine deployments (e.g., mounting a second disk partition and sharing it between containers).

## Directory Structure

Within your shared storage volume (at the path configured as `shared_dir` on each node), create the following essential directory structure. Ensure these directories are readable and writable by the user(s) running HakuRiver components on the Host and Runners.

```bash
# Assuming your shared_dir is /mnt/hakuriver-shared on this node:
mkdir -p /mnt/hakuriver-shared/hakuriver-containers
mkdir -p /mnt/hakuriver-shared/task_outputs
mkdir -p /mnt/hakuriver-shared/task_errors
mkdir -p /mnt/hakuriver-shared/shared_data

# Set permissions (adjust based on your user/group running HakuRiver)
# Example: allow read/write for the HakuRiver user/group, read-only for others
sudo chown -R hakuriver_user:hakuriver_group /mnt/hakuriver-shared # Replace user/group
sudo chmod -R 775 /mnt/hakuriver-shared # Example: User/Group can R/W/X, Others can R/X

# Specific permissions for log directories may need to be more permissive
# if tasks running as different users need to write there, or stricter if security is paramount.
# By default, Runner writes logs as its user. Ensure this user has write access.
sudo chmod 777 /mnt/hakuriver-shared/task_outputs # Example: allow all to write logs
sudo chmod 777 /mnt/hakuriver-shared/task_errors # Example: allow all to write logs
```

The resulting structure should look like this within your `shared_dir`:

```
<shared_dir>/
├── hakuriver-containers/  # HakuRiver Docker image tarballs are stored here by the Host
├── task_outputs/          # Standard output logs (*.out) for Command tasks are written here by Runners
├── task_errors/           # Standard error logs (*.err) for Command tasks are written here by Runners
└── shared_data/           # Conventional location for user data/scripts (mounted as /shared in Docker tasks)
```

## Configuration on Different Nodes

As mentioned, the `shared_dir` path in the `config.toml` file on each Host and Runner node must be set to its *local mount point* for the shared storage volume.

**Host config** (`~/.hakuriver/config.toml` on the Host):
```toml
[paths]
shared_dir = "/mnt/hakuriver-shared-on-host" # Replace with the Host's mount point
```

**Runner config** (`~/.hakuriver/config.toml` on EACH Runner):
```toml
[paths]
shared_dir = "/mnt/shared-on-runner" # Replace with THIS Runner's mount point
```

The `container_dir` setting in `[docker]` defines the subdirectory *within* `shared_dir` where tarballs are kept and must be consistent across Host and Runners.

## Testing Configuration

After setting up shared storage on all nodes and configuring `shared_dir` in their respective `config.toml` files, verify that shared storage is working correctly across all nodes:

1.  Create a test file on the Host within its configured `shared_dir`:
    ```bash
    echo "test file from host" > /mnt/hakuriver-shared-on-host/testfile.txt # Adjust path
    ```

2.  Verify it's visible on all Runners at their configured `shared_dir` paths:
    ```bash
    # On Runner 1 (adjust path)
    cat /mnt/shared-on-runner/testfile.txt

    # On Runner 2 (adjust path)
    cat /shared_storage-on-runner/testfile.txt
    ```

3.  Create a test file on a Runner within its configured `shared_dir`:
    ```bash
    # On Runner 1 (adjust path)
    echo "test file from runner1" > /mnt/shared-on-runner/runner1-test.txt
    ```

4.  Verify it's visible on the Host and other Runners:
    ```bash
    # On Host (adjust path)
    cat /mnt/hakuriver-shared-on-host/runner1-test.txt

    # On Runner 2 (adjust path)
    cat /shared_storage-on-runner/runner1-test.txt
    ```
If these tests pass, your basic shared storage setup for HakuRiver is correct.

## Performance Considerations

### Container Tarball Distribution

Container tarballs can be large (hundreds of MB to a few GB). When a Runner needs to load an image that's not cached locally, it reads the tarball from shared storage. This process can be a bottleneck. Consider these optimization strategies:

-   Use a high-bandwidth network (e.g., 10GbE) between your storage server(s) and compute nodes.
-   Ensure your storage server and network configuration are optimized for large file reads.
-   Use compression-friendly base images in your Docker environment setup to reduce tarball size.

### Log File Access

Task logs are continuously written to `shared_dir/task_outputs/` and `shared_dir/task_errors/` by Runners. The Host reads these files when requested via the Web UI or CLI.

-   Ensure your storage can handle concurrent writes from multiple Runners.
-   NFS with the `async` option can improve write performance but risks data loss on server failure. Use `sync` for safer, but potentially slower, writes.
-   Consider setting up log rotation for the output/error directories to prevent them from consuming excessive disk space over time (see Maintenance below).

### Data Access Patterns

Consider your workload's data access patterns within the `/shared` mount (which maps to `shared_dir/shared_data/`):

-   **Read-heavy**: Optimize read performance (e.g., caching, read-ahead).
-   **Write-heavy**: Ensure sufficient write bandwidth.
-   **Random access**: SSD-backed storage will perform significantly better than HDD.

## Monitoring and Maintenance

### Monitoring Shared Storage

Monitor the shared storage volume for capacity, performance, and errors:

```bash
# Check disk usage on a node where it's mounted
df -h /mnt/hakuriver-shared # Adjust path

# Check disk IO on the storage server or a client
iostat -x 5 /dev/sdX # Replace with actual device name

# Check network throughput between a client and storage server (example with iperf)
# Run iperf server on storage host: iperf -s
# Run iperf client on compute node: iperf -c storage-server-ip
```

### Maintenance Tasks

Regular maintenance tasks for shared storage:

1.  **Cleanup old tarballs**: HakuRiver automatically removes older tarballs for the *same container name* when a new one is created. However, you may need manual cleanup for tarballs of decommissioned environments.
2.  **Log rotation**: Set up log rotation for task output logs (`*.out`, `*.err`) in `shared_dir/task_outputs` and `shared_dir/task_errors`. This prevents log files from filling up your storage over time, especially with numerous or long-running tasks.

    ```bash
    # Example logrotate config file: /etc/logrotate.d/hakuriver
    /mnt/hakuriver-shared/task_outputs/*.out { # Adjust path
        weekly
        rotate 4
        compress
        delaycompress
        missingok
        notifempty
        create 0664 runner_user runner_group # Adjust user/group
    }
    /mnt/hakuriver-shared/task_errors/*.err { # Adjust path
        weekly
        rotate 4
        compress
        delaycompress
        missingok
        notifempty
        create 0664 runner_user runner_group # Adjust user/group
    }
    ```

3.  **Check file permissions** periodically to ensure HakuRiver processes still have read/write access where needed.

## Backup Strategies

Consider these backup options for the critical data on your shared storage:

1.  **Container tarball backup**: The `shared_dir/hakuriver-containers/` directory contains your reproducible environments. Backing this up allows you to restore environments.
2.  **Task log backup**: Depending on your needs, you might want to back up logs for historical analysis or debugging.
3.  **Shared data backup**: Back up any user data or scripts stored in the shared directory.

Backup methods can range from simple `rsync` jobs to snapshot-based backups (ZFS, LVM, cloud provider snapshots) or more complex backup software.

## Troubleshooting

### Common Issues

-   **Mount Failures:** Check network connectivity, firewall on the storage server, and the storage service status (e.g., `sudo systemctl status nfs-kernel-server`). Verify syntax in `/etc/fstab`.
-   **Permission Issues:** Check file ownership and permissions on the shared directory and its subdirectories. Verify mount options (e.g., `no_root_squash` for NFS root access, `uid`/`gid` mapping). Ensure the user running HakuRiver processes has the necessary permissions.
-   **Performance Problems:** Monitor network bandwidth and disk IO on the storage server and clients. Identify if the bottleneck is network or storage I/O.
-   **NFS Stale File Handles:** Often resolved by remounting the filesystem (`sudo umount <mount_point> && sudo mount <mount_point>`). Check NFS server health if it persists.
-   **Task failures due to missing files:** Verify the expected input files exist in the shared directory at the path your task expects (`/shared/input/` etc.) and that the container user has read access.

## Advanced Configuration

### High Availability Setup

For production environments, consider high-availability shared storage solutions like:

-   GlusterFS with replica volumes.
-   Ceph with replicated pools.
-   NFS with failover servers (using clustering software like Pacemaker/Corosync).

### Performance Tuning

Tuning network filesystem mounts can improve performance. Consult documentation for your specific filesystem (NFS, CIFS, GlusterFS) for recommended mount options.

Example NFS mount options for performance (use with caution, test thoroughly):
```
nfs-server:/export/hakuriver /mnt/hakuriver-shared nfs defaults,rw,hard,intr,rsize=131072,wsize=131072,noatime,async 0 0
```

## Next Steps

After setting up shared storage correctly on all nodes:

1.  Ensure the [Host is configured](1. host-setup.md) to use the shared directory.
2.  Ensure all [Runners are configured](2. runner-setup.md) with proper shared storage access.
3.  Review and implement relevant [Security Measures](5. security.md) for your storage solution and overall cluster.
4.  Test the entire setup by submitting tasks that access data on the shared storage and verify logs appear correctly.