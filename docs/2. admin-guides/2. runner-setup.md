# Runner Setup Guide

This guide provides detailed instructions for setting up and configuring HakuRiver Runner nodes. Runners execute tasks distributed by the Host server, so proper setup is essential for reliable operation, including Docker, Systemd (optional fallback), and potential GPU support.

## Hardware Requirements

Runner hardware requirements depend on the workloads you plan to run. Ensure nodes meet or exceed the resource requests of the tasks they will execute.

### Minimum Requirements
- **CPU**: 2+ cores
- **RAM**: 4GB+
- **Disk**:
  - 20GB+ for the Runner software and Docker installation.
  - Local temporary storage (`local_temp_dir`).
  - Access to shared storage for container tarballs and logs.

### Recommended for Compute-Intensive Workloads
- **CPU**: 8+ cores, potentially with NUMA architecture.
- **RAM**: 16GB+.
- **Disk**: Fast local SSD/NVMe for the temporary directory (`local_temp_dir`).
- **GPU(s)**: If you plan to run GPU-accelerated tasks.
- **Network**: 1Gbps+ connectivity to the Host and shared storage.

## Operating System Requirements

HakuRiver Runner has been tested on:
- Ubuntu 20.04/22.04/24.04 LTS
- Debian 10/11/12
- CentOS 7/8
- RHEL 7/8/9
- Should work on any Linux distribution with Python 3.10+ and Docker.

## Software Prerequisites

Install the following software on **each Runner node**:

### Python Installation

HakuRiver requires Python 3.10 or newer:

```bash
# Ubuntu/Debian
sudo apt update
sudo apt install python3 python3-pip python3-venv

# CentOS/RHEL 8+
sudo dnf install python3 python3-pip

# Check version
python3 --version
```

If your distribution doesn't provide Python 3.10+, consider using pyenv or installing from source.

### Docker Installation

Runners require Docker Engine for executing containerized Command and VPS tasks:

```bash
# Example for Ubuntu/Debian (adjust for your distribution)
sudo apt update
sudo apt install -y apt-transport-https ca-certificates curl gnupg lsb-release
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
echo "deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt update
sudo apt install -y docker-ce docker-ce-cli containerd.io

# Add the user who will run the Runner process to the docker group
sudo usermod -aG docker $USER
# You will need to log out and log back in for the group change to apply

# Start and enable Docker service
sudo systemctl start docker
sudo systemctl enable docker

# Check Docker installation and permissions
docker run hello-world
```

#### Docker Configuration

Considerations for `data-root`:
- Choose a location with sufficient disk space for storing all the container images that tasks might use.
- Prefer SSD for better performance.

For storage-driver:
- `overlay2` is recommended for most modern Linux distributions.

Verify Docker configuration:
```bash
docker info | grep -E 'Storage Driver|Docker Root Dir'
```

### NUMA and Systemd Tools Installation (Optional)

These are required if you plan to use the systemd fallback execution method (`--container NULL` for command tasks) or NUMA targeting features (`--target node:numa_id`).

```bash
# Ubuntu/Debian
sudo apt install -y numactl systemd-container # systemd-container provides systemd-run

# CentOS/RHEL
sudo dnf install -y numactl systemd
```

### NVIDIA Drivers and pynvml (Optional)

These are required if you plan to run GPU-accelerated tasks or want runners to report GPU usage/details.

1.  Install appropriate NVIDIA drivers for your GPU and OS. Follow NVIDIA's documentation.
2.  Install the `pynvml` Python library. This is installed automatically if you use the `[gpu]` extra during HakuRiver installation.

```bash
# If you installed HakuRiver without [gpu] before
python -m pip install "hakuriver[gpu]"

# Verify pynvml can see your GPUs (run this outside the runner service)
# python -c "import pynvml; pynvml.nvmlInit(); print(f'Detected {pynvml.nvmlDeviceGetCount()} GPUs'); pynvml.nvmlShutdown()"
```

## HakuRiver Installation

Install HakuRiver on **each Runner node**.

### Option 1: Install via pip

```bash
# Create a Python virtual environment (recommended)
python3 -m venv ~/.venv/hakuriver
source ~/.venv/hakuriver/bin/activate

# Install HakuRiver (use [gpu] if you need GPU support)
pip install hakuriver
# pip install "hakuriver[gpu]"
```

### Option 2: Install from Source

```bash
# Clone repository
git clone https://github.com/KohakuBlueleaf/HakuRiver.git
cd HakuRiver

# Create and activate virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install (use [gpu] if needed)
pip install -e .
# pip install -e ".[gpu]"
```

## Runner Configuration

### Create Configuration Directory and Default File

```bash
# Initialize config (creates ~/.hakuriver/config.toml if it doesn't exist)
hakuriver.init config
```

This creates a default configuration file at `~/.hakuriver/config.toml`.

### Edit Configuration

```bash
vim ~/.hakuriver/config.toml
```

#### Critical Runner Configuration Settings

Configure these settings on **each Runner node**:

```toml
# ~/.hakuriver/config.toml on EACH Runner node

[network]
# Address of the Host server (accessible from this Runner).
host_reachable_address = "192.168.1.100" # <--- CHANGE THIS (Same as Host's config)

# Port the Host API is listening on.
host_port = 8000 # <--- CHANGE THIS IF NEEDED (Same as Host's config)

# Port the Host SSH proxy is listening on. Not strictly needed by Runner,
# but useful to have consistent config across nodes.
# host_ssh_proxy_port = 8002 # Optional: <--- CHANGE THIS IF NEEDED (Same as Host's config)

# The address of *this specific Runner* (accessible from the Host).
# IMPORTANT: Must be a unique, actual IP or hostname for THIS machine.
runner_address = "192.168.1.101" # <--- CHANGE THIS (MUST BE UNIQUE PER RUNNER)

# Port this Runner will listen on for commands from the Host.
runner_port = 8001 # <--- CHANGE THIS IF NEEDED

[paths]
# Absolute path to shared storage. This must point to the same physical storage as on the Host,
# but the path configured here is THIS Runner's mount point for that storage.
shared_dir = "/mnt/shared/hakuriver" # <--- CHANGE THIS (Runner's mount point)

# Absolute path to local temporary storage on THIS Runner node.
local_temp_dir = "/tmp/hakuriver-temp" # <--- CHANGE THIS IF NEEDED

# Path to the numactl executable on THIS Runner node. Required for Systemd+NUMA.
numactl_path = "/usr/bin/numactl" # <--- CHANGE THIS IF NEEDED OR LEAVE "" TO DISABLE NUMA+Systemd

[environment]
# Optional: Specify the system username under which tasks should be run by the Runner.
# If empty, tasks run as the user who started the `hakuriver.runner` process.
# If specified, Runner may need passwordless sudo to run tasks as this user.
runner_user = "" # <--- SET THIS IF YOU WANT TASKS TO RUN AS A SPECIFIC USER

[docker]
# The relative path within shared_dir where HakuRiver container tarballs are stored.
container_dir = "hakuriver-containers" # (Same as Host's config)

# Default setting for whether tasks should run privileged. Can be overridden by Host/task.
tasks_privileged = false # (Same as Host's config)

# Default additional mounts for tasks. Overridden by Host/task if specified.
additional_mounts = [] # (Same as Host's config)
```

See the [Configuration Reference](../4. reference/1. configuration.md) for all available options.

### Create Directories

Ensure all necessary directories exist on the Runner node and are writable by the user running the Runner process:

```bash
# Create local temp directory
sudo mkdir -p /tmp/hakuriver-temp
sudo chown $USER:$USER /tmp/hakuriver-temp # Change $USER to the user running the service

# Create shared directory mount point (if shared storage is mounted here)
sudo mkdir -p /mnt/shared/hakuriver
sudo chown $USER:$USER /mnt/shared/hakuriver # Ensure Runner user can read/write here
```

## Shared Storage Setup

The Runner needs access to shared storage for:
- Reading container tarballs created by the Host (`shared_dir/container_dir`).
- Writing task output logs (`shared_dir/task_outputs`, `shared_dir/task_errors`).
- Reading/writing shared task input/output data (`shared_dir/shared_data`).

The logical path (`shared_dir`) must match the Runner's mount point for the shared filesystem. This path can be different from the Host's `shared_dir`, but must point to the same physical storage.

See [Shared Storage Guide](3. shared-storage.md) for detailed setup instructions, including mounting options and verification.

### Testing Shared Storage on the Runner

Verify shared storage is correctly mounted and writable by the Runner user:

```bash
# Check mount status
df -h /mnt/shared/hakuriver # Adjust path

# Check permissions
ls -la /mnt/shared/hakuriver # Adjust path

# Create/Write test file
echo "Runner test file" > /mnt/shared/hakuriver/runner-test-file.txt # Adjust path
cat /mnt/shared/hakuriver/runner-test-file.txt # Should show "Runner test file"
rm /mnt/shared/hakuriver/runner-test-file.txt

# Verify file exists from Host/other nodes if possible
```

## Systemd and Sudo Permissions (for Systemd fallback and Docker access)

The Runner may need to execute tasks using `sudo systemd-run` (for `--container NULL` command tasks) and potentially requires `sudo` for Docker commands depending on your Docker user setup.

Configure `sudo` permissions for the user running the Runner process to allow executing specific commands without a password. **Use `visudo` to edit the sudoers file.**

```bash
# Edit sudoers file (as root or with sudo)
sudo visudo

# Add lines similar to these, replacing 'runner_username' with the actual user
# You can restrict the commands further if needed, but these are typically required:
runner_username ALL=(ALL) NOPASSWD: /usr/bin/systemd-run, /usr/bin/systemctl, /usr/bin/docker
```
*Note:* The specific path to `docker` might vary (`/usr/bin/docker`, `/usr/local/bin/docker`, etc.). Find the correct path using `which docker`. Restricting `systemctl` might be needed for status checks or other sub-commands used by the runner.

See [Systemd Integration Guide](4. systemd-integration.md) for more detailed systemd setup and permission configuration.

## Starting the Runner Agent

### Running Manually

For testing or debugging, run the Runner agent in the foreground:

```bash
# Activate virtual environment if used
source ~/.venv/hakuriver/bin/activate

# Start the Runner
hakuriver.runner

# Start with a specific config file
# hakuriver.runner --config /path/to/custom-config.toml
```

### Running as Systemd Service (Recommended)

For production use, configure and run the Runner as a systemd service:

```bash
# Create systemd service file (adjust --config if not using default path)
hakuriver.init service --runner

# Copy the generated service file to the systemd directory
sudo cp hakuriver-runner.service /etc/systemd/system/

# Reload systemd daemon to recognize the new service file
sudo systemctl daemon-reload

# Start the service
sudo systemctl start hakuriver-runner.service

# Enable the service to start automatically on boot
sudo systemctl enable hakuriver-runner.service

# Check the service status
sudo systemctl status hakuriver-runner.service
```

See [Systemd Integration Guide](4. systemd-integration.md) for more details. Ensure the service user has the correct permissions and environment setup (e.g., PATH for `numactl`).

## Verifying Runner Registration

After starting the Runner, verify it has successfully registered and is online with the Host. From the **Host machine** or any **Client machine**:

```bash
hakuriver.client nodes
```

The Runner should appear in the list with status "online". If it's offline, check the Runner's systemd logs (`journalctl -u hakuriver-runner.service`), its configuration (`runner_address` vs `host_reachable_address`, ports), and network connectivity to the Host.

## NUMA Configuration (Optional)

If your compute nodes have NUMA architecture and you intend to use NUMA targeting for systemd tasks:

1.  Ensure the `numactl` package is installed (`sudo apt install numactl` or `sudo dnf install numactl`).
2.  Ensure the `numactl_path` is correctly set in the Runner's `config.toml`.
3.  Verify `sudo` permissions for the runner user to execute `numactl`.

## Multiple Runner Instances on the Same Machine (Advanced)

For specific use cases (e.g., dedicating different resources on one machine to different Runner instances), you can run multiple Runners on the same host. Each instance requires:

- A separate configuration file (`runner_port` and `runner_address` must be unique).
- Potentially separate working directories and local temp directories.
- Separate systemd service files (e.g., `hakuriver-runner-8001.service`, `hakuriver-runner-8002.service`).

## Firewall Configuration

Configure the runner's firewall to allow incoming connections from the Host on the configured `runner_port` (default 8001):

```bash
# Ubuntu/Debian with UFW
sudo ufw allow from <host_ip_address> to any port <runner_port> proto tcp
# Allow SSH for management
sudo ufw allow ssh
# Deny other incoming by default (optional but recommended)
# sudo ufw default deny incoming
sudo ufw enable
```

## Runner Management

- Check status: `sudo systemctl status hakuriver-runner.service`
- Check logs: `sudo journalctl -u hakuriver-runner.service`
- Restart: `sudo systemctl restart hakuriver-runner.service`
- Stop: `sudo systemctl stop hakuriver-runner.service`

## Troubleshooting

### Common Issues

- **Registration Failure:** Check Host status, network connectivity from Runner to Host, `host_reachable_address`/`host_port` in Runner config, and Runner's systemd logs.
- **Docker Errors:** Ensure Docker service is running, runner user is in the `docker` group, and Docker has disk space.
- **Shared Storage Access:** Verify mount status, permissions, and `shared_dir` in Runner config.
- **Systemd Permission Issues:** Verify `sudo visudo` configuration for the runner user allows `NOPASSWD` for `systemd-run`, `systemctl`, and `docker`.
- **Task Execution Failures:** Check task status and logs (`hakuriver.client status <task_id>`, `hakuriver.task stdout/stderr <task_id>`). Ensure required container tarball exists in shared storage and can be loaded. Check Runner systemd logs for errors during task launch.

For more detailed troubleshooting, see the [Troubleshooting Guide](../troubleshooting/common-issues.md).

## Performance Optimization

- **Local Temp Directory:** Use fast storage (SSD/NVMe) for `local_temp_dir`.
- **Docker Image Caching:** Runners automatically cache images, but pre-pulling frequently used base images can help (`docker pull <image>`).
- **NUMA Binding:** For compute-intensive workloads, use NUMA targeting (`--target node:numa_id`) in task submissions to improve memory locality.
- **GPU Setup:** Ensure NVIDIA drivers are correctly installed and CUDA/cuDNN are configured within the container environment used for GPU tasks.

## Next Steps

After setting up Runner nodes:

1.  Confirm [Shared Storage](3. shared-storage.md) is correctly configured and accessible on all nodes.
2.  Ensure [Systemd Services](4. systemd-integration.md) are set up for reliable operation.
3.  Review [Security Measures](5. security.md) for your cluster.
4.  Submit test tasks (Command and VPS) from a client to verify functionality across different runners and configurations.