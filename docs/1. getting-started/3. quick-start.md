# Quick Start Guide

This guide will help you quickly set up HakuRiver and run your first distributed tasks. We'll cover the essential steps to get a working cluster up and running with basic Command and VPS tasks.

## 1. Setup Overview

A HakuRiver cluster consists of:
- One **Host Node** (central manager)
- One or more **Runner Nodes** (compute nodes)
- Shared storage accessible by all nodes
- Client tools for submitting tasks (can be on Host or a separate machine)

## 2. Prerequisites

Before proceeding, ensure:
- HakuRiver is [installed](2. installation.md) on all nodes (Host, Runners, Clients).
- Docker is running on the Host and all Runners, and the user running HakuRiver can access it.
- Shared storage is mounted at the same logical path on all nodes (e.g., `/mnt/shared/hakuriver`).
- All nodes can reach each other over the network on the configured API ports.
- Clients can reach the Host on the configured API and SSH proxy ports.

## 3. Configure HakuRiver

On **each machine** (Host, Runners, and Clients), create the default configuration file and edit it:

```bash
# Create default config
hakuriver.init config

# Edit configuration
vim ~/.hakuriver/config.toml
```

Modify the critical settings, ensuring they reflect your network setup and shared storage path:

```toml
# ~/.hakuriver/config.toml on Host, Runners, and Clients

[network]
# On ALL machines, set this to the Host node's IP/hostname reachable by others
host_reachable_address = "192.168.1.100" # Replace with your Host's actual IP

# On EACH Runner, set this to the Runner's OWN IP/hostname reachable by the Host
# NOT needed on Host or Client machines.
# runner_address = "192.168.1.101" # Replace with this Runner's actual IP

# The port the Host will listen on for the SSH proxy (for VPS). Clients need this.
# Set on Host and Client machines.
host_ssh_proxy_port = 8002

[paths]
# On ALL machines, set to the ABSOLUTE path of your shared storage
# This must be the same physical storage, but path can differ per node (configure accordingly)
shared_dir = "/mnt/shared/hakuriver" # Replace with your shared path
```
Adjust `host_port` (default 8000) and `runner_port` (default 8001) if needed. For Runners, configure `local_temp_dir`.

## 4. Start the Host Server

On your designated **Host node**:

```bash
# Start in the foreground (for testing - allows seeing logs directly)
hakuriver.host

# OR use systemd for production (recommended)
# hakuriver.init service --host
# sudo systemctl start hakuriver-host.service
# sudo systemctl enable hakuriver-host.service # Enable auto-start on boot
```

The Host will initialize its database, configure Docker access, start the SSH proxy, and prepare the default Docker container tarball in shared storage if it doesn't exist.

## 5. Start Runner Agents

On **each compute node** that will act as a Runner:

```bash
# Ensure Docker is running and the user can access it.
# Ensure shared_dir and local_temp_dir exist and are writable.
# If using GPUs, ensure pynvml is installed and drivers are loaded.

# Start in the foreground (for testing)
hakuriver.runner

# OR use systemd for production (recommended)
# hakuriver.init service --runner
# sudo systemctl start hakuriver-runner.service
# sudo systemctl enable hakuriver-runner.service # Enable auto-start on boot
```

Runners will register with the Host, report their resources (CPU, RAM, NUMA, GPU), and start sending heartbeats.

## 6. Verify Node Registration

From any machine with client access (Host, Client machine):

```bash
# List registered nodes
hakuriver.client nodes
```

You should see all your Runner nodes listed with status "online". If a node is offline, check its configuration, network connectivity to the Host, and the Runner's systemd logs (`journalctl -u hakuriver-runner.service`).

## 7. Prepare a Docker Environment (Optional but Recommended)

HakuRiver comes with a default container environment (`hakuriver-base`) based on a minimal Python image. You can use this directly, or create and customize your own environments for specific software or dependencies. This is done on the **Host** machine, but usually triggered from a **Client** machine via `hakuriver.docker` commands which talk to the Host.

```bash
# On a Client machine

# Create a new persistent base container on the Host (e.g., using an Ubuntu image)
hakuriver.docker create-container ubuntu:22.04 my-ubuntu-env

# Start an interactive shell to install packages inside the container on the Host
# This opens a web-based terminal session through the Host
hakuriver.docker-shell my-ubuntu-env
# Now you are inside the container's shell on the Host
# Example: Install openssh-server and a text editor
# apt-get update && apt-get install -y openssh-server nano
# For VPS: set up SSH for root user (e.g., enable PermitRootLogin in /etc/ssh/sshd_config if disabled, add your public key to /root/.ssh/authorized_keys)
# Ensure sshd is configured to run (often done automatically on install, but check)
# exit # Type exit to leave the container shell

# Create a distributable tarball from the modified container state
# This saves the image to shared_dir/hakuriver-containers/
hakuriver.docker create-tar my-ubuntu-env

# Verify the tarball exists
hakuriver.docker list-tars
```
This customized environment (`my-ubuntu-env`) is now ready for task execution on any Runner. When a Runner needs this environment, it will automatically load the tarball.

## 8. Submit Your First Tasks

Submit both a Command Task and a VPS Task.

**Submit a Command Task** (e.g., run a simple script or command):
```bash
# On a Client machine

# Submit a simple echo command using the default container to node1
hakuriver.task submit --target node1 -- echo "Hello, HakuRiver Command Task!"

# Submit a Python script using your custom env on node2 with 2 cores
# (Assumes myscript.py is in the shared dir, accessible via /shared in container)
# Add the shared directory as a mount: /mnt/shared/hakuriver:/shared
# (or use the default mounts if configured)
# Note: You may need to configure this mount globally in config.toml or per task
hakuriver.task submit --target node2 --cores 2 --container my-ubuntu-env -- python /shared/myscript.py --input /shared/data.csv

# Submit a GPU command task to node3 using GPU 0 and 1 (requires GPU-enabled node/container)
hakuriver.task submit --target node3::0,1 --container my-gpu-env -- python /shared/train_gpu_model.py

# Submit a system command directly on node4 (Systemd fallback, no Docker)
hakuriver.task submit --target node4 --container NULL -- df -h /
```
Note the `--` separator before the command and its arguments for `hakuriver.task submit`.

**Submit a VPS Task** (launch an interactive environment):
```bash
# On a Client machine

# Launch a VPS on node1 using your custom container (prepared with SSH) and 1 core
# Provide your public key. By default, looks for ~/.ssh/id_rsa.pub or ~/.ssh/id_ed25519.pub
hakuriver.vps submit --target node1 --cores 1 --container my-ubuntu-env

# Launch a VPS using GPU 0 on node3 with auto-selected core/memory (cores 0 = auto)
# Requires a container prepared with SSH and CUDA drivers if needed
hakuriver.vps submit --target node3::0 --cores 0 --container my-gpu-env --public-key-file ~/.ssh/my_gpu_key.pub
```

## 9. Monitor and Manage Tasks

Use the client commands to check status and manage tasks.

```bash
# On a Client machine

# List active VPS tasks (shows task ID, status, assigned node, SSH port)
hakuriver.vps status

# Check status of a specific task (any type - Command or VPS)
hakuriver.client status <task_id_from_submission_or_list>

# View standard output (only for Command tasks)
hakuriver.task stdout <command_task_id>

# View standard error (only for Command tasks)
hakuriver.task stderr <command_task_id>

# Pause a task (Command or VPS, if supported by runner execution method)
hakuriver.client command <task_id> pause

# Resume a paused task (Command or VPS)
hakuriver.client command <task_id> resume

# Kill a task (Command or VPS)
hakuriver.client kill <task_id>
```

## 10. Connect to a Running VPS Task

Once a VPS task is running, use its Task ID to connect via SSH through the Host proxy:

```bash
# On a Client machine (where hakuriver.ssh is installed and config points to Host)

# Connect to a running VPS task (replace with your VPS task ID)
hakuriver.ssh <vps_task_id>
# Example: hakuriver.ssh 7323718749301768192

# You should now be prompted for your SSH key password (if your key is protected)
# and then dropped into the container's shell as root (if configured).
```

## 11. Explore the Web Dashboard (Optional)

If you have set up and deployed the optional web dashboard (often served by the Host or a separate web server):

1.  Navigate to the dashboard URL in your browser (e.g., `http://<host-ip>:<host-port>`).
2.  Explore the "Nodes", "GPUs", "Tasks" (for Command tasks), and "VPS" pages.
3.  Submit tasks using the web forms.
4.  View task details, logs, and manage running tasks visually.

## Next Steps

Now that you have a basic HakuRiver cluster running and can submit both Command and VPS tasks, you can:

- Learn more about the [Docker Container Workflow](../3. user-guides/1. container-workflow.md) for preparing environments.
- Explore [Command Task Submission](../3. user-guides/2. command-tasks/1. submission.md) options.
- Learn more about [VPS Task Management](../3. user-guides/3. vps-tasks/1. management.md) and [SSH Access](../3. user-guides/3. vps-tasks/2. ssh-access.md).
- Understand [GPU Allocation](../3. user-guides/4. gpu-allocation/1. allocation.md) in detail.
- Utilize the [HakuRun Utility](../3. user-guides/7. hakurun-utility/1. utility.md) for parameter sweeps.
- Set up the [Web Dashboard](../3. user-guides/5. web-dashboard/1. overview.md) for visual management.
- Dive deeper into [Monitoring](../3. user-guides/6. monitoring/1. monitoring.md).