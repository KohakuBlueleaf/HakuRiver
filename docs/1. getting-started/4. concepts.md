# Core Concepts

This document explains the fundamental concepts and design principles behind HakuRiver. Understanding these concepts will help you make the most of the system.

## Docker as a Portable Environment

### Traditional vs. HakuRiver Approach

In traditional container orchestration (like Kubernetes) or simple Docker Compose setups:
- Containers primarily package and deploy applications or services.
- Each service often runs in its own container, configured with specific networking, ports, etc.
- The focus is on deploying scalable, often long-running applications.

In HakuRiver:
- Containers serve as **portable virtual environments**.
- The primary focus is on providing consistent and reproducible execution environments for **CLI tasks** and **interactive sessions (VPS)**.
- The same container image (`hakuriver/<env_name>:base`) can be used for many different transient command tasks or long-running VPS tasks across various nodes.
- Containers are managed by HakuRiver's specific workflow and automatically distributed/synced.

> **Key Insight**: Docker in HakuRiver functions more like a familiar development environment (like Python's `venv` or Conda) that can be dynamically prepared on the Host and automatically synchronized to execute the *same command or environment* on different compute nodes, ensuring consistency.

## Host-Runner Architecture

HakuRiver uses a simple, centralized Host-Runner architecture:

![HakuRiver Flow](../../image/README/HakuRiverFlow.jpg)

### The Host (`hakuriver.host`)
- **Role:** Central coordinator, API server, state manager.
- **Responsibilities:**
    - Manages the cluster's overall state (registered nodes, tasks, etc.) in its SQLite database.
    - Exposes the main API for clients and runners.
    - Receives task submissions (Command and VPS).
    - Performs basic scheduling (validates targets, allocates resources like cores/memory/GPUs if requested, selects node if target is auto-selected).
    - Dispatches tasks to the appropriate Runner agent.
    - Tracks node status via heartbeats and marks nodes as offline.
    - Manages the Host-side Docker containers used for environment *preparation*.
    - Packages prepared environments into tarballs in shared storage.
    - Runs the **SSH proxy server** to enable secure SSH access to VPS tasks.
    - Serves the optional web dashboard.

### The Runners (`hakuriver.runner`)
- **Role:** Agent running on each compute node, responsible for executing tasks locally.
- **Responsibilities:**
    - Registers with the Host on startup, reporting its resources (CPU, RAM, **NUMA topology**, **GPU details**).
    - Sends periodic heartbeats with current resource usage and status of local tasks.
    - Receives task execution requests from the Host.
    - **Automatically synchronizes** the required Docker container image from shared storage if it's missing or outdated.
    - **Executes tasks**:
        - **Docker Mode (Default):** Runs the command or starts the VPS container using `docker run`, applying resource limits (--cpus, --memory, --gpus), mounting shared/local/additional directories, and configuring the environment (SSH for VPS, etc.). Uses `--rm` for command tasks (clean up after exit) and `--restart unless-stopped` for persistent VPS tasks.
        - **Systemd Fallback (`--container NULL` for command tasks):** Runs the command directly on the node using `sudo systemd-run --scope`, applying resource limits (CPUQuota, MemoryMax) and optionally NUMA binding (`numactl`).
    - Reports task status updates (running, completed, failed, killed, paused, resumed, etc.) back to the Host.
    - Handles kill/pause/resume signals from the Host by interacting with `docker` or `systemctl`.

### The Clients (`hakuriver.*` CLI tools and Web Dashboard)
- **Role:** Provide interface for users to interact with the Host.
- **Responsibilities:**
    - Submit Command tasks (`hakuriver.task submit`).
    - Submit VPS tasks (`hakuriver.vps submit`), including providing public key and specifying targets/resources.
    - Manage any task (status, kill, pause/resume) by ID via `hakuriver.client command <task_id> <action>`.
    - Monitor nodes and cluster health (`hakuriver.client nodes`, `hakuriver.client health`).
    - Manage Host-side Docker environments (`hakuriver.docker`, `hakuriver.docker-shell`).
    - Connect to running VPS tasks via the SSH proxy (`hakuriver.ssh <vps_task_id>`).
    - The Web Dashboard provides a graphical interface for many of these actions.

### Communication Flow
1.  Runners start, detect resources, and register with the Host (`POST /register`).
2.  Runners periodically send resource usage, temperature, and running/killed task lists to the Host (`PUT /heartbeat/{hostname}`).
3.  Clients (CLI or Web UI) send task submission requests (Command or VPS) to the Host (`POST /submit`).
4.  The Host selects a suitable Runner (based on targeting and availability), creates a task record in its DB, and sends the task details to the chosen Runner's `/run` endpoint.
5.  The Runner receives the task, checks/syncs the required Docker image from Shared Storage, starts the task execution (Docker or Systemd), and reports status updates back to the Host (`POST /update`).
6.  Clients query the Host for task status (`GET /status/{task_id}`), node lists (`GET /nodes`), cluster health (`GET /health`), etc.
7.  Clients can request to kill/pause/resume tasks by calling Host endpoints (`POST /kill/{task_id}`, `POST /command/{task_id}/pause`, `POST /command/{task_id}/resume`). The Host then forwards the request to the assigned Runner.
8.  For SSH access to a VPS, the Client (`hakuriver.ssh`) connects to the Host's SSH proxy port. The Host proxy looks up the VPS task ID in the DB to find the assigned Runner's IP and the dynamically assigned SSH port, then tunnels the SSH connection directly to the Runner/VPS container.

## Container Workflow

HakuRiver's Docker-based environment management follows a specific workflow:

1.  **Prepare Base Environment (on Host):**
    - A user creates a persistent Docker container on the Host machine using `hakuriver.docker create-container` from a standard image (e.g., `ubuntu`, `python`).
    - They can then get an interactive shell into this container (`hakuriver.docker-shell`) to install software, libraries, configure SSH (for VPS), etc., just like setting up a development environment.
    - This persistent container is the "master" environment copy.

2.  **Package the Environment (on Host):**
    - Once the environment is ready, the user uses `hakuriver.docker create-tar`.
    - This command commits the *current state* of the persistent Host container into a new Docker image (`hakuriver/<env_name>:base`) and exports that image as a versioned tarball (`<env_name>-<timestamp>.tar`) into the configured shared storage directory (`shared_dir/container_dir`). Older tarballs for the same environment are pruned.

3.  **Automatic Synchronization (on Runners):**
    - When a task (Command or VPS) is assigned to a Runner that requires a specific container environment (`--container <env_name>`), the Runner first checks its *local* Docker image store.
    - It compares the local image's timestamp (if it exists) with the timestamp of the *latest* tarball for that environment in shared storage.
    - If the local image is missing or older, the Runner automatically loads the latest tarball into its local Docker registry using `docker load`. This ensures Runners always use the most recent version of the environment.

4.  **Task Execution (on Runner):**
    - The Runner executes the task command or starts the VPS container using `docker run`.
    - For **Command Tasks**, it creates a *temporary* container from the synced `hakuriver/<env_name>:base` image using `docker run --rm ...`.
    - For **VPS Tasks**, it launches a *persistent* container (`docker run -d --restart unless-stopped ...`), configures SSH access within it (injecting the public key, starting `sshd`), and maps a random host port to container port 22, reporting this host port back to the Host.
    - Resource limits (CPU, Memory, GPU) and configured directory mounts (`/shared`, `/local_temp`, plus additional mounts) are applied to the container.

This workflow ensures tasks run in consistent, pre-configured environments across all nodes without requiring manual setup on each Runner node beyond installing Docker and relevant drivers (like for GPUs) and ensuring required directories exist.

## Task Submission and Targeting

HakuRiver offers flexible task targeting options for both Command and VPS tasks.

### Target Syntax (`--target` flag in `hakuriver.task submit` and `hakuriver.vps submit`)
- `node1`: Target a specific node (auto-selects core/NUMA/GPU on that node).
- `node1:0`: Target a specific NUMA node (ID 0) on `node1`. Primarily for Systemd tasks.
- `node1::0`: Target a specific GPU (ID 0) on `node1`. Only for Docker/VPS tasks.
- `node1::0,1,3`: Target multiple specific GPUs (IDs 0, 1, and 3) on `node1`. Only for Docker/VPS tasks.

### Multi-Target vs. Single-Target
- **Command Tasks (`hakuriver.task submit`):** Can use the `--target` flag multiple times (`--target node1 --target node2:0 --target node3::0`). This creates a separate instance of the task for *each* specified target, all part of the same batch. Resource requirements are per instance.
- **VPS Tasks (`hakuriver.vps submit`):** Only accept a *single* `--target` flag. A VPS runs on one specific node, potentially bound to one NUMA node or given access to one set of GPUs on that node. Omitting `--target` allows the Host to auto-select a suitable node based on requested resources.

### Execution Methods on Runner
- **Docker Mode (Default):** Task runs inside a Docker container created from a HakuRiver managed environment tarball (`--container <env_name>`). This is the primary method, used for reproducible environments, resource limits, GPU access, and VPS tasks.
- **Systemd Fallback (`--container NULL`):** The Command task (and *only* Command tasks) can be run directly on the Runner host OS using `sudo systemd-run --scope`. Resource limits and NUMA binding are applied via systemd properties and `numactl`. This is useful for tasks needing direct host access or where Docker is not suitable/available for that specific task (though Docker is required for environment sync).

### Resource Allocation
- `--cores N`: Request N CPU cores. Used by the Host for scheduling and by the Runner (Docker `--cpus`, Systemd `CPUQuota`).
- `--memory SIZE`: Set a memory limit (e.g., `4G`). Used by the Host for scheduling and by the Runner (Docker `--memory`, Systemd `MemoryMax`).
- `--target node::gpu_ids`: Request specific GPUs. Used by the Host for scheduling and by the Runner (Docker `--gpus`).
- `--target node:numa_id`: Request NUMA binding. Used by the Host for scheduling and by the Runner (Systemd `numactl --cpunodebind --membind`).

## SSH Proxy for VPS Tasks

HakuRiver includes a built-in SSH proxy managed by the Host.
- **Purpose:** Provides a single, stable SSH entry point to your cluster for accessing running VPS tasks, even if the Runner nodes have dynamic IPs, are behind NAT, or have complex network setups.
- **How it Works:**
    1. The Client runs `hakuriver.ssh <vps_task_id>`.
    2. The `hakuriver.ssh` client connects to the Host's SSH proxy port (`host_ssh_proxy_port`).
    3. It sends a request asking the Host to tunnel for the specified `<vps_task_id>`.
    4. The Host looks up the task in its DB, finds its assigned Runner node and the dynamically assigned SSH port mapped to the VPS container.
    5. The Host proxy establishes a connection to the Runner's IP and the VPS container's SSH port.
    6. The Host proxy then acts as a simple data forwarder, piping the SSH connection data between the client running `hakuriver.ssh` and the actual SSH daemon inside the VPS container on the Runner.
    7. The user's SSH client authenticates directly with the VPS container's SSH daemon using the public key injected during VPS submission.

This allows users to connect to any running VPS with a single command, `hakuriver.ssh <task_id>`, without needing to know the specific Runner's IP or the dynamic port assigned by Docker.

## Shared Storage and Mounting

Shared storage is a critical, mandatory component:

- **Container Tarballs**: Created by the Host, stored in `shared_dir/container_dir/`. Runners automatically pull these when needed.
- **Task Logs**: Standard output and error logs for Command tasks are written directly by the Runner to `shared_dir/task_outputs/` and `shared_dir/task_errors/`.
- **Shared Data**: Any data or scripts placed in `shared_dir/shared_data/` (or another location configured via mounts) are accessible to tasks.

Every Docker container launched by the Runner automatically gets these bind mounts:
- `/shared`: Maps to `shared_dir/shared_data/` (or a configured shared data path).
- `/local_temp`: Maps to the Runner's `local_temp_dir`.
- `/usr/bin/numactl`, `/usr/lib/x86_64-linux-gnu/`, etc.: Required system binaries/libraries are automatically mounted into the container if the Systemd fallback is used for *any* task on that Runner, to ensure potentially needed tools like `numactl` are available even in minimal base images.

These mounts provide tasks with access to shared inputs/outputs and local scratch space, regardless of which Runner they run on.

## Task Lifecycle

Tasks in HakuRiver follow a lifecycle tracked by the Host:

1.  **Submitted:** Client sends request to Host.
2.  **Pending:** Host receives and validates request.
3.  **Assigning:** Host selects a Runner and sends the task details.
4.  **Running:** Runner starts execution and reports 'running' status. Logs (for command tasks) and SSH port (for VPS tasks) become available.
5.  **Paused:** Runner receives pause signal from Host and attempts to pause execution.
6.  **Resumed:** Runner receives resume signal from Host and attempts to resume execution.
7.  **Completed:** Task finishes successfully (exit code 0).
8.  **Failed:** Task finishes with a non-zero exit code or encounters a Runner-side error (e.g., container failed to start, script error).
9.  **Killed:** User/Client explicitly requests termination.
10. **Killed_OOM:** Runner reports task was killed by the OS/Docker due to Out-of-Memory.
11. **Lost:** The assigned Runner goes offline before the task finishes.

The Host updates the Task record in its SQLite database as status changes, allowing users to query the current state, view logs (for command tasks), and get connection info (for VPS tasks).

## The `hakurun` Utility

HakuRiver includes a standalone utility called `hakurun` for parameter sweeps:

- **Purpose**: Test commands with multiple argument combinations *locally* before submitting them to the cluster.
- **Usage**: Run locally, does **not** interact with the HakuRiver cluster directly.
- **Syntax**: Uses `span:{start..end}` for integer ranges and `span:[a,b,c]` for lists of items.
- **Execution**: Runs all resulting command combinations sequentially or in parallel as local subprocesses.
- **Integration with HakuRiver**: You can use `hakurun` in two ways with HakuRiver:
    1. Generate commands locally with `hakurun`, then pipe them to `hakuriver.task submit` (e.g., `hakurun echo "hakuriver.task submit ..." | bash`). This creates separate cluster tasks for each combination.
    2. Include `hakurun` as the command *within* a single HakuRiver task (`hakuriver.task submit ... hakurun --parallel ...`). This runs the entire sweep on one Runner node.

## Web Dashboard

The optional web dashboard provides a visual interface for:

- **Monitoring**: View node status, resource usage (CPU, Memory, **GPU**), and task status lists (**Command** and **VPS**).
- **Submission**: Submit new **Command** and **VPS tasks** through detailed forms supporting all parameters (targets, resources, container, **GPU selection**, **public key**).
- **Management**: Kill, pause, or resume tasks via buttons.
- **Logs**: View standard output and error logs for Command tasks directly in the browser.
- **Docker Management**: View/manage Host containers and shared tarballs.
- **VPS Details**: See assigned node, resources, status, and **SSH port** for VPS tasks.

## Next Steps

Now that you understand the core concepts of HakuRiver, you can:

- Follow the [Quick Start Guide](3. quick-start.md) to set up your cluster.
- Read about the [Docker Container Workflow](../3. user-guides/1. container-workflow.md) in detail, including preparing VPS containers.
- Explore [Command Task Submission](../3. user-guides/2. command-tasks/1. submission.md).
- Learn about [VPS Task Management](../3. user-guides/3. vps-tasks/1. management.md) and [SSH Access](../3. user-guides/3. vps-tasks/2. ssh-access.md).
- Understand [GPU Allocation](../3. user-guides/4. gpu-allocation/1. allocation.md).