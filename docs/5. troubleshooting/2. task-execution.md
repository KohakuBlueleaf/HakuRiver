# Troubleshooting Task Execution Failures

This guide focuses on diagnosing and resolving issues that prevent HakuRiver tasks (both Command and VPS) from launching successfully or completing as expected after they have been accepted and dispatched by the Host.

Before using this guide, ensure you have checked the [Common Issues Guide](1. common-issues.md) for basic setup and connectivity problems.

## Task Status: 'Failed' or 'Killed_OOM'

A task ending in 'failed' or 'killed_oom' indicates a problem occurred during its execution on the assigned Runner node.

**Diagnosis Steps:**

1.  **Check Task Status Details:**
    *   Use `hakuriver.client status <task_id>` or the Web Dashboard Task/VPS Details view.
    *   Note the `status` (`failed`, `killed_oom`).
    *   Note the `exit_code` (if available). `0` is success, any non-zero is failure. Specific codes like `137` (-9) often indicate OOM kill from Docker.
    *   Note the `error_message`. HakuRiver or the Runner might add a message here (e.g., "Killed by runner: oom", "Docker image load failed", "Runner rejected task", "Assignment failed").
    *   Note the `assigned_node`. You'll need to check logs on this specific Runner.
    *   Note the `container_name` or `systemd_unit_name`. This tells you if it was a Docker or Systemd task and its specific unit name.
    *   Note the `required_cores`, `required_memory_bytes`, `required_gpus`.

2.  **Check Task Logs (Command Tasks Only):**
    *   If it's a **Command task** and `status` is 'failed', retrieve and review the standard error log: `hakuriver.task stderr <task_id>` or use the Web Dashboard.
    *   Review the standard output log as well: `hakuriver.task stdout <task_id>` or Web Dashboard.
    *   These logs contain the output from your script or command running *inside* the container or systemd scope. Look for error messages printed by your code or the commands it runs.

3.  **Check Runner Service Logs:**
    *   Log into the `assigned_node`.
    *   Check the systemd logs for the Runner service: `sudo journalctl -u hakuriver-runner.service`. Filter by timestamp around when the task failed.
    *   Look for messages related to the task ID or its container/unit name (`hakuriver-task-<task_id>` or `hakuriver-vps-<task_id>`).
    *   Runner logs errors encountered when attempting to launch Docker or Systemd commands, or when receiving kill signals (like OOM).

4.  **Check Specific Execution Engine Logs:**
    *   If it was a **Docker task** (Command or VPS):
        *   Check Docker daemon logs on the Runner: `sudo journalctl -u docker.service`. Look for errors related to `docker run`, `docker create`, or container exits for the task's container name.
        *   For a **VPS task** (`hakuriver-vps-<task_id>`), the container is persistent. You can check its logs directly on the Runner: `docker logs hakuriver-vps-<task_id>`. This shows stdout/stderr of the container's entrypoint (usually the SSH daemon setup/start).
    *   If it was a **Systemd fallback task**:
        *   Check logs for the transient systemd scope unit: `sudo journalctl -u hakuriver-task-<task_id>.scope`. This captures logs *from* `systemd-run` itself and potentially early output/errors from the task process before redirection to files.

**Common Scenarios and Solutions:**

*   **"Docker image load failed" / Image Not Found:**
    *   **Logs:** Runner logs show errors loading the tarball or Docker reporting "No such image".
    *   **Cause:** The container tarball (`<container_name>-<timestamp>.tar`) is missing from `shared_dir/container_dir/` on the Runner, or the Runner doesn't have read permission for the file. Or, the Runner attempted to load it but the `docker load` command failed.
    *   **Solution:** Verify the tarball exists on the shared storage. Check `shared_dir` setting and permissions on the Runner. Check Runner logs for `docker load` errors. Ensure the container name used in the task submission (`--container NAME`) matches the name in shared storage tarballs.
*   **"Command not found" or Script Missing:**
    *   **Logs:** Task `stderr` or Runner logs show "command not found", "No such file or directory", or similar when trying to execute the main task command.
    *   **Cause:** The executable or script specified in `hakuriver.task submit -- COMMAND` does not exist at that path *inside the container* (for Docker tasks) or *on the Runner host* (for Systemd tasks). Often, this is due to forgetting to mount the shared directory, using an incorrect path, or the script not being in the container image/host OS.
    *   **Solution:** Ensure your script/executable is in the shared directory (`shared_dir/shared_data/`) or included in the container image. Verify mounts are correctly applied (`/shared` mapping `shared_dir/shared_data/`). Use the container path (e.g., `/shared/my_script.sh`).
*   **Permission Denied:**
    *   **Logs:** Task `stderr` or Runner logs show "Permission denied".
    *   **Cause:** The user executing the task (either the default container user, the `runner_user` for Systemd, or the user running the Runner service) does not have permission to access files, directories, or devices needed by the task. For Systemd tasks, this often relates to `sudo systemd-run` permissions or accessing host resources without sufficient privileges.
    *   **Solution:** Check filesystem permissions on input/output files and directories in shared/local storage. If using Systemd fallback, review `sudo visudo` configuration for the runner user. If accessing sensitive host resources, consider if `--privileged` is truly needed (use with extreme caution).
*   **OOM Kill (`killed_oom` status):**
    *   **Logs:** Task status is `killed_oom`. Runner logs or `sudo journalctl` might show kernel messages about OOM killer for the task's process/container PID.
    *   **Cause:** The task attempted to use more memory than was available or more than its `required_memory_bytes` limit (enforced by Docker or Systemd).
    *   **Solution:** Increase the `--memory SIZE` limit for the task, or run on a node with more total/available memory. Optimize the task's memory usage.
*   **Non-Zero Exit Code (Status 'failed'):**
    *   **Logs:** Task status is 'failed', `exit_code` is non-zero. Task `stderr` has the program's error output.
    *   **Cause:** The command or script executed finished but returned a non-zero exit status, indicating an application-level error.
    *   **Solution:** Debug the task's logic using its `stdout` and `stderr` logs. The problem is with your code or the environment it ran in, not typically HakuRiver itself.
*   **systemd-run or Docker Launch Errors:**
    *   **Logs:** Runner logs show errors from `subprocess.run` attempting to execute `systemd-run` or `docker run`. Error message might mention command not found, syntax errors in the generated command, or Docker API errors.
    *   **Cause:** Problems with the Runner's setup (systemd-run not found, `numactl_path` incorrect, Docker daemon issue), or HakuRiver generated an invalid command.
    *   **Solution:** Verify Runner prerequisites (Docker, systemd, numactl, sudo). Check Runner logs for the exact command being attempted by HakuRiver and manually try running it (as the runner user with appropriate `sudo`) to reproduce the error.
*   **GPU Allocation Errors:**
    *   **Logs:** Task 'failed' on startup, error message mentions GPUs, devices, or NVIDIA runtime. Runner logs show `docker run --gpus...` errors.
    *   **Cause:** NVIDIA Container Toolkit not configured for Docker on the Runner, requested GPU IDs are invalid/unavailable, or the container image lacks necessary libraries or is incompatible with the driver.
    *   **Solution:** Verify NVIDIA Container Toolkit installation/configuration (`docker run --rm --gpus all nvidia/cuda:base nvidia-smi`). Check GPU IDs requested against `hakuriver.client health <node>`. Ensure container image has compatible CUDA/cuDNN libs.
*   **Task in 'Assigning' with High Suspicion Count:**
    *   **Logs (Host):** Host logs warn about suspicion count increasing for the task.
    *   **Logs (Runner):** Runner logs might show the task *attempted* to start but failed immediately after `systemd-run`/`docker run`, before reporting 'running'. Or the Runner process might be unstable and not sending updates.
    *   **Cause:** The Host dispatched the task, but the Runner never reported it as 'running' via a heartbeat or update. This can happen if the Runner process crashed, the task failed immediately on launch, or network communication from Runner back to Host is broken.
    *   **Solution:** Check Runner service status and logs. Check network from Runner back to Host. Investigate task launch failure causes (previous points in this guide).

## Task Status: 'Lost'

A task status of 'lost' means the assigned Runner node stopped reporting heartbeats while the task was in 'running' or 'assigning' status.

**Diagnosis Steps:**

1.  **Check Node Status:** Use `hakuriver.client nodes`. The assigned node will be marked 'offline'.
2.  **Check Runner Logs:** Log into the assigned node and check the Runner service logs (`journalctl -u hakuriver-runner.service`) and system status (`htop`, `uptime`). Find out why the Runner process stopped or why the node went offline.
3.  **Check Network:** Verify network connectivity between the Host and the Runner.

**Solution:** Troubleshoot the Runner node's service or network connectivity to bring it back online. The 'lost' task cannot be recovered; it must be re-submitted.

## Next Steps

-   Use the [Monitoring Guide](../6. monitoring/1. overview.md) to actively track task statuses and node health.
-   Consult the [Interpreting Logs Guide](../6. monitoring/2. interpreting-logs.md) for help understanding log messages.
-   Review the [Command Task Submission Guide](../2. command-tasks/1. submission.md) and [VPS Task Management Guide](../3. vps-tasks/1. management.md) to ensure tasks are submitted correctly.