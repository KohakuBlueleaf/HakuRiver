# Integration Guide

Integrate KohakuRiver with external tools and workflows.

---

## External Monitoring

### Prometheus Integration

KohakuRiver doesn't have a built-in Prometheus exporter, but you can create one:

```python
# Example: scrape kohakuriver status
import requests
from prometheus_client import Gauge, start_http_server

nodes_online = Gauge('kohakuriver_nodes_online', 'Number of online nodes')
tasks_running = Gauge('kohakuriver_tasks_running', 'Number of running tasks')

def collect_metrics():
    resp = requests.get('http://localhost:8000/nodes')
    nodes = resp.json()
    nodes_online.set(sum(1 for n in nodes if n['status'] == 'online'))

    resp = requests.get('http://localhost:8000/tasks?status=running')
    tasks_running.set(len(resp.json()))

# Run exporter on port 9100
start_http_server(9100)
```

### Grafana Dashboards

Use the Prometheus exporter above or parse CLI output:

```bash
# Scripted metric collection
kohakuriver node list --json | jq '.[] | select(.status=="online")' | wc -l
```

### Nagios/Zabbix

Create check scripts:

```bash
#!/bin/bash
# check_kohakuriver_host.sh
curl -sf http://localhost:8000/health > /dev/null
if [ $? -eq 0 ]; then
    echo "OK - KohakuRiver Host is running"
    exit 0
else
    echo "CRITICAL - KohakuRiver Host is down"
    exit 2
fi
```

---

## Workflow Managers

### Snakemake Integration

```python
# Snakefile
rule process_data:
    input: "data/{sample}.csv"
    output: "results/{sample}.json"
    shell:
        """
        kohakuriver task submit "python /shared/process.py {input} {output}" \
            -t node1 --container data-env --wait
        """
```

### Nextflow Integration

```groovy
// nextflow.config
process {
    executor = 'local'
}

// main.nf
process runTask {
    input:
    path input_file

    script:
    """
    kohakuriver task submit "python /shared/analyze.py ${input_file}" \
        -t node1 --container analysis-env --wait
    """
}
```

### Airflow Integration

```python
from airflow import DAG
from airflow.operators.bash import BashOperator

dag = DAG('kohakuriver_pipeline', ...)

task = BashOperator(
    task_id='run_analysis',
    bash_command='''
        kohakuriver task submit "python /shared/analyze.py" \
            -t node1 --container my-env --wait
    ''',
    dag=dag,
)
```

---

## CI/CD Integration

### GitHub Actions

```yaml
# .github/workflows/run-on-cluster.yml
jobs:
  cluster-job:
    runs-on: self-hosted
    steps:
      - name: Submit task
        run: |
          kohakuriver task submit "python /shared/ci/run_tests.py" \
            -t node1 --container test-env --wait
```

### GitLab CI

```yaml
# .gitlab-ci.yml
test:
  script:
    - kohakuriver task submit "pytest /shared/tests/" \
        -t node1 --container test-env --wait
```

---

## Notifications

### Slack Notifications

```bash
#!/bin/bash
# notify_slack.sh
TASK_ID=$1
STATUS=$(kohakuriver task status $TASK_ID --json | jq -r '.status')

curl -X POST -H 'Content-type: application/json' \
    --data "{\"text\":\"Task $TASK_ID: $STATUS\"}" \
    $SLACK_WEBHOOK_URL
```

### Email Notifications

```python
#!/usr/bin/env python3
# notify_email.py
import smtplib
import subprocess
import json

task_id = sys.argv[1]
result = subprocess.run(
    ['kohakuriver', 'task', 'status', task_id, '--json'],
    capture_output=True, text=True
)
status = json.loads(result.stdout)

# Send email...
```

### Webhook Integration

```bash
# Generic webhook on task completion
kohakuriver task submit "python /shared/job.py && curl -X POST $WEBHOOK_URL" \
    -t node1 --container my-env
```

---

## API Integration

### REST API

The Host exposes a REST API at port 8000:

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | Health check |
| `/nodes` | GET | List nodes |
| `/tasks` | GET | List tasks |
| `/tasks` | POST | Submit task |
| `/tasks/{id}` | GET | Task details |
| `/tasks/{id}/kill` | POST | Kill task |
| `/vps` | GET | List VPS |
| `/vps` | POST | Create VPS |

### Python Client Example

```python
import requests

HOST = "http://localhost:8000"

# List nodes
nodes = requests.get(f"{HOST}/nodes").json()

# Submit task
task = requests.post(f"{HOST}/tasks", json={
    "command": "python /shared/script.py",
    "targets": ["node1"],
    "container_name": "my-env",
    "required_cores": 4,
}).json()

# Check status
status = requests.get(f"{HOST}/tasks/{task['task_id']}").json()
```

---

## Complementary Tools

| Tool | Use With KohakuRiver |
|------|----------------------|
| **Ansible** | Initial node setup, Docker install |
| **Terraform** | Cloud infrastructure provisioning |
| **Snakemake/Nextflow** | Workflow orchestration |
| **Prometheus/Grafana** | External monitoring |
| **NFS/GlusterFS** | Shared storage backend |

---

## Best Practices

| Practice | Description |
|----------|-------------|
| **Use `--wait` for scripts** | Ensures task completion before next step |
| **Check exit codes** | Script integration should check return values |
| **Log task IDs** | Save task IDs for later reference |
| **Handle failures** | Build retry logic into integrations |
