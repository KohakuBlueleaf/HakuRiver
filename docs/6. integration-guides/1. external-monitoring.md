# Integrating with External Monitoring

HakuRiver provides API endpoints (`/api/nodes`, `/api/health`, `/api/tasks`, `/api/vps/status`) that expose cluster state and resource usage. Administrators can leverage these endpoints to integrate HakuRiver data into external monitoring systems like Prometheus+Grafana, Nagios, Zabbix, or custom dashboards.

This guide provides concepts and examples for pulling data, but setting up the external monitoring system itself is outside the scope of HakuRiver documentation.

## Accessing HakuRiver Data

The primary way to get HakuRiver data for external monitoring is by querying the Host's REST API.

**Host API Base URL:** `http://<host_reachable_address>:<host_port>/api`

Ensure your monitoring system has network access to the Host on this address and port, potentially through a firewall.

### Key Endpoints for Monitoring Data

1.  **`/api/health`**: Provides detailed, historical (last ~60 seconds) health metrics including CPU %, Memory %, temperature, and GPU stats for all nodes, plus cluster-wide aggregates.
    *   Method: `GET`
    *   Parameters: Optional `hostname=<node_name>` to filter.
    *   Response: JSON object containing `nodes` (array of historical node snapshots) and `aggregate` (array of historical aggregate snapshots). The latest data is the last element in each array.
    *   *Use Case:* General cluster health dashboard, node resource graphs, temperature monitoring, GPU health/utilization graphs.
2.  **`/api/nodes`**: Provides a list of all registered nodes with their status (online/offline), total/available cores, NUMA summary, and GPU summary.
    *   Method: `GET`
    *   Response: JSON array of node objects.
    *   *Use Case:* Node inventory, online/offline counts, checking available core capacity.
3.  **`/api/tasks`**: Provides a list of all **Command tasks** known to the Host, with their status, assigned node, resources, timestamps, etc.
    *   Method: `GET`
    *   Response: JSON array of task objects.
    *   *Use Case:* Dashboard task list, counting tasks by status (running, pending, failed), identifying long-running tasks.
4.  **`/api/vps/status`**: Provides a list of all **active VPS tasks** (pending, assigning, running, paused) with their status, assigned node, resources, and SSH port.
    *   Method: `GET`
    *   Response: JSON array of VPS task objects.
    *   *Use Case:* Dashboard active VPS list, counting active VPS, monitoring VPS status.

## Data Collection Methods

### 1. Scripting and Polling

Write scripts (e.g., Python, Bash) that periodically call the HakuRiver API endpoints using tools like `curl` or `httpx` (Python). The script then processes the JSON response and:

-   Pushes data to a time-series database (e.g., Prometheus via Pushgateway, InfluxDB).
-   Writes data to a file that your monitoring agent can scrape (e.g., Node Exporter textfile collector).
-   Sends alerts based on thresholds (e.g., using `mailx`, Slack webhooks) if a critical condition is detected (e.g., nodes offline, many failed tasks).

**Example Python Script (using httpx) to fetch health:**

```python
import httpx
import json
import sys
import os # To read config path from env if needed

# Assuming Host URL is set in a config file or environment variable
# For simplicity, hardcode or read from config
HAKURIVER_HOST_URL = "http://your_hakuriver_host:8000/api" # CHANGE THIS

def get_cluster_health():
    try:
        response = httpx.get(f"{HAKURIVER_HOST_URL}/health", timeout=10)
        response.raise_for_status() # Raise an exception for bad status codes
        data = response.json()
        # The latest data is the last element in the aggregate array
        latest_aggregate = data["aggregate"][-1] if data["aggregate"] else {}
        latest_nodes = data["nodes"][-1] if data["nodes"] else []
        return latest_aggregate, latest_nodes
    except httpx.RequestError as e:
        print(f"Error fetching health from {HAKURIVER_HOST_URL}: {e}", file=sys.stderr)
        return None, None
    except Exception as e:
        print(f"An unexpected error occurred: {e}", file=sys.stderr)
        return None, None

if __name__ == "__main__":
    aggregate, nodes = get_cluster_health()

    if aggregate:
        print("--- Aggregate Health ---")
        print(f"Online Nodes: {aggregate.get('onlineNodes')}/{aggregate.get('totalNodes')}")
        print(f"Avg CPU Util: {aggregate.get('avgCpuPercent'):.1f}%")
        print(f"Avg Mem Util: {aggregate.get('avgMemPercent'):.1f}%")
        print(f"Max CPU Temp: {aggregate.get('maxMaxCpuTemp'):.1f}C")
        # You would then format this data for your specific monitoring system
        # For Prometheus textfile collector:
        # print(f'hakuriver_online_nodes {aggregate.get("onlineNodes")}')
        # print(f'hakuriver_total_nodes {aggregate.get("totalNodes")}')
        # print(f'hakuriver_avg_cpu_percent {aggregate.get("avgCpuPercent")}')
        # print(f'hakuriver_avg_mem_percent {aggregate.get("avgMemPercent")}')
        # print(f'hakuriver_max_cpu_temp {aggregate.get("maxMaxCpuTemp")}')

    if nodes:
        print("\n--- Node Health Summary ---")
        for node in nodes:
             print(f"Node: {node.get('hostname')}, Status: {node.get('status')}, CPU: {node.get('cpu_percent'):.1f}%, Mem: {node.get('memory_percent'):.1f}%")
             if node.get('gpu_info'):
                 print("  GPUs:")
                 for gpu in node['gpu_info']:
                     print(f"    GPU {gpu.get('gpu_id')}: {gpu.get('name')}, Util: {gpu.get('gpu_utilization') or 0}%, Temp: {gpu.get('temperature') or 0}C")
```
This script can be scheduled via `cron` to periodically fetch data.

### 2. Custom Exporter (for Prometheus)

For Prometheus monitoring, the ideal approach is often a custom *exporter*. An exporter is a small service that runs alongside the monitored application (or on the monitoring server) and exposes metrics in a format that Prometheus can scrape.

A HakuRiver exporter would:
1.  Periodically call the HakuRiver API endpoints (`/api/health`, `/api/nodes`, etc.).
2.  Convert the data into Prometheus metric format (e.g., `my_metric_name{label="value"} value`).
3.  Expose these metrics on an HTTP endpoint (e.g., `/metrics`) that Prometheus is configured to scrape.

This requires more development effort but provides a more robust and standard integration with Prometheus. You could potentially adapt the Python script approach into a simple web server exposing the data.

### 3. Monitoring Agent Plugins

Some monitoring systems (e.g., Zabbix, Nagios) use agents installed on nodes or a central server. You could write a plugin for your agent that specifically knows how to query the HakuRiver API and report status or metrics.

## Metrics to Collect

Based on the API endpoints, useful metrics for external monitoring include:

**Cluster-wide (from `/api/health` aggregate):**
-   `hakuriver_total_nodes` (Gauge)
-   `hakuriver_online_nodes` (Gauge)
-   `hakuriver_total_cores` (Gauge)
-   `hakuriver_total_memory_bytes` (Gauge)
-   `hakuriver_used_memory_bytes` (Gauge)
-   `hakuriver_avg_cpu_percent` (Gauge)
-   `hakuriver_avg_memory_percent` (Gauge)
-   `hakuriver_max_avg_cpu_temp` (Gauge)
-   `hakuriver_max_max_cpu_temp` (Gauge)
-   `hakuriver_tasks_total` (Gauge) - Count all tasks (from `/api/tasks` and `/api/vps/status` or `/api/status` for all IDs)
-   `hakuriver_tasks_status_total` (Gauge with label `status`) - Count tasks by status (`pending`, `assigning`, `running`, `completed`, `failed`, `killed`, `lost`, `paused`)
-   `hakuriver_vps_active_total` (Gauge) - Count active VPS tasks (from `/api/vps/status`)
-   `hakuriver_vps_status_total` (Gauge with label `status`) - Count active VPS by status

**Per-Node (from `/api/health` nodes or `/api/nodes`), labelled by `hostname`:**
-   `hakuriver_node_status` (Gauge, 1 for online, 0 for offline)
-   `hakuriver_node_total_cores` (Gauge)
-   `hakuriver_node_available_cores` (Gauge)
-   `hakuriver_node_cores_in_use` (Gauge)
-   `hakuriver_node_total_memory_bytes` (Gauge)
-   `hakuriver_node_used_memory_bytes` (Gauge)
-   `hakuriver_node_available_memory_bytes` (Gauge)
-   `hakuriver_node_cpu_percent` (Gauge)
-   `hakuriver_node_memory_percent` (Gauge)
-   `hakuriver_node_numa_nodes_total` (Gauge)
-   `hakuriver_node_current_avg_temp` (Gauge)
-   `hakuriver_node_current_max_temp` (Gauge)

**Per-GPU (from `/api/health` nodes or `/api/nodes`), labelled by `hostname` and `gpu_id`:**
-   `hakuriver_gpu_utilization_percent` (Gauge)
-   `hakuriver_gpu_memory_utilization_percent` (Gauge)
-   `hakuriver_gpu_memory_total_mib` (Gauge)
-   `hakuriver_gpu_memory_used_mib` (Gauge)
-   `hakuriver_gpu_temperature_celsius` (Gauge)
-   `hakuriver_gpu_power_usage_mw` (Gauge)
-   `hakuriver_gpu_power_limit_mw` (Gauge)
-   `hakuriver_gpu_graphics_clock_mhz` (Gauge)
-   `hakuriver_gpu_memory_clock_mhz` (Gauge)

## Alerting Scenarios

Based on collected metrics, you can set up alerts for conditions like:

-   A Runner node goes offline.
-   A high percentage of nodes are offline.
-   High cluster-wide CPU or memory utilization (approaching capacity).
-   High CPU temperature on a node.
-   Many tasks are stuck in 'pending' or 'assigning' status.
-   A significant number of tasks are in 'failed', 'lost', or 'killed' status.
-   High GPU utilization or temperature on specific nodes/GPUs.
-   A critical VPS task goes into a non-running state.

## Security Considerations

-   If your monitoring system is external to the trusted HakuRiver network segment, use HTTPS for API calls and potentially client certificate authentication or API keys if HakuRiver or a reverse proxy is configured to support them.
-   Restrict network access to the API endpoints used for monitoring (primarily GET requests) via firewalls.

## Next Steps

-   Identify your monitoring system requirements.
-   Choose a data collection method (scripting, exporter, agent plugin).
-   Write/configure the data collection component to query the HakuRiver Host API.
-   Set up dashboards and alerts in your monitoring system using the collected metrics.
-   Consult the full [API Reference](../4. reference/6. api-reference.md) for exact endpoint paths and response structures.