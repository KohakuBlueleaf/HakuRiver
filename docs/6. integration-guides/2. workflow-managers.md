# Integrating with External Workflow Managers

HakuRiver is designed for task execution and environment management, but it is not a full workflow manager. Complex workflows involving task dependencies (e.g., Task B starts only after Task A completes successfully, and Task C starts only after Tasks A and B complete) are best handled by dedicated workflow management systems.

You can integrate workflow managers like Snakemake, Nextflow, Airflow, or Luigi by configuring them to **submit tasks to HakuRiver** instead of executing them directly or submitting to traditional HPC schedulers.

## The Integration Pattern

The general pattern for integrating an external workflow manager with HakuRiver is:

1.  **Workflow Manager Definition:** Define your complex workflow, including task dependencies, data flow, and execution logic, within your chosen workflow manager's framework (e.g., `Snakefile`, `.nf` script, Airflow DAG).
2.  **Execution Backend:** Configure the workflow manager to use a custom execution backend or profile that wraps the execution of individual tasks within the workflow using the `hakuriver.task submit` command.
3.  **HakuRiver Task Submission:** For each step in the workflow that needs to run on the cluster, the workflow manager constructs and executes a `hakuriver.task submit` command on the client machine (or a submission node).
    -   It uses the `--target` option to specify where the task should run.
    -   It uses the `--container` option to specify the required HakuRiver environment.
    -   It passes the specific command and arguments for that workflow step.
    -   **Crucially, it uses the `--wait` flag** on the `hakuriver.task submit` command. This makes the workflow manager process wait until HakuRiver reports the task as completed (or failed/killed), allowing the workflow manager to correctly manage dependencies before proceeding to the next step.
4.  **Status Monitoring:** The workflow manager relies on the exit code of the `hakuriver.task submit --wait` command to determine if the task succeeded or failed. Non-zero exit codes from `hakuriver.task submit --wait` (indicating the submitted task failed, was killed, or the submission itself failed) signal the workflow manager to handle the failure according to its logic.
5.  **Data Management:** Data dependencies between workflow steps should typically be handled via the shared storage (`shared_dir`) mounted into the HakuRiver tasks (`/shared`). The workflow manager defines paths relative to the shared storage for inputs and outputs.

## Examples with Specific Workflow Managers

### Snakemake

Snakemake can execute rules using various cluster environments. You can create a cluster configuration that uses `hakuriver.task submit`.

1.  **Create a Snakemake Profile/Cluster Config:**
    Create a file (e.g., `cluster_hakuriver.yaml`) defining how Snakemake should submit jobs.
    ```yaml
    __default__:
      # Base HakuRiver submit command
      submit: "hakuriver.task submit --wait --target {cluster.target} --cores {threads} --memory {cluster.memory} --container {cluster.container} -- {exec}"
      # Add any default mounts, privileged, etc. here if needed
      # mount: "--mount /data:/data"

    rule_specific_rule:
      # Override defaults for a specific rule if needed
      target: "gpu-node::0"
      container: "my-gpu-env"
      memory: "16G"
      # mount: "--mount /large_scratch:/scratch"
    ```
2.  **Specify Resources in `Snakefile`:** In your `Snakefile`, define resources for each rule using the `threads` and `resources` directives. Use the keys defined in your cluster config (`target`, `memory`, `container`, etc.).
    ```python
    rule process_data:
        input: "data/input.csv"
        output: "results/processed.csv"
        threads: 4
        resources:
            target="nodeA",
            memory="8G",
            container="my-data-env"
        shell:
            "my_processor --input {input} --output {output} --threads {threads}" # Command run inside the container

    rule train_model:
        input: "results/processed.csv"
        output: "model/trained.pkl"
        threads: 8 # Number of cores for the task
        resources:
            target="gpu-node::0", # Target a specific GPU
            memory="16G",
            container="my-gpu-env" # Use a GPU-ready container
        shell:
            "python train.py --data {input} --output {output} --device 0" # Command run inside the container
    ```
3.  **Run Snakemake:** Execute Snakemake, pointing it to your cluster config file.
    ```bash
    # Ensure hakuriver is in your PATH and configured correctly on the submission machine
    snakemake --snakefile Snakefile --profile cluster_hakuriver.yaml --jobs <max_concurrent_tasks>
    ```
    Snakemake will then submit each rule that needs execution as a separate HakuRiver task using the command defined in the profile. Snakemake's dependency management will wait for tasks to complete before submitting dependent rules.

### Nextflow

Nextflow has robust support for various execution platforms. You can configure an executor to use `hakuriver.task submit`.

1.  **Create a Nextflow Config:** Define a process executor block that runs commands via `hakuriver.task submit`.
    ```groovy
    // nextflow.config
    process {
        executor = 'hakuriver' # Define a custom executor name

        // Default process options translated to HakuRiver args
        queue = 'default' // Use this to map to a default target or auto-select logic
        cpus = 1
        memory = '2 GB'
        container = 'hakuriver-base' // Default container

        // Custom properties accessible via task.ext.xyz
        ext {
            hakuriver_target = null // Default target
            hakuriver_container = null // Allow overriding container per process
            hakuriver_gpus = null // List of GPU IDs
            hakuriver_mounts = [] // Additional mounts
        }
    }

    // Define the custom executor
    executors {
        hakuriver {
            name = 'HakuRiver Executor'
            // The command executed for each process
            script = '''
                #!/bin/bash
                set -euo pipefail

                # Ensure hakuriver command is available (activate venv if needed)
                # source /path/to/your/venv/bin/activate # Adjust if using venv

                # Construct the hakuriver.task submit command dynamically
                HAKURIVER_CMD="hakuriver.task submit --wait"

                # Add target if specified
                if [ "${task.ext.hakuriver_target}" != "null" ]; then
                    HAKURIVER_CMD="${HAKURIVER_CMD} --target ${task.ext.hakuriver_target}"
                fi

                # Add CPU/Memory/Container/Mounts/GPUs
                HAKURIVER_CMD="${HAKURIVER_CMD} --cores ${task.cpus}"
                HAKURIVER_CMD="${HAKURIVER_CMD} --memory '${task.memory}'" # Quote memory to handle units
                if [ "${task.ext.hakuriver_container}" != "null" ]; then
                   HAKURIVER_CMD="${HAKURIVER_CMD} --container '${task.ext.hakuriver_container}'"
                else
                   HAKURIVER_CMD="${HAKURIVER_CMD} --container '${process.container}'" # Use default container
                fi

                if [ "${task.ext.hakuriver_gpus}" != "null" ]; then
                    # Assuming ext.hakuriver_gpus is a string like "0,1" or similar
                    HAKURIVER_CMD="${HAKURIVER_CMD} --target \"\$(hostname)\"::$(echo ${task.ext.hakuriver_gpus} | tr -d '[] ')" # Basic parsing, might need refinement
                fi

                # Add mounts
                for mount in "${task.ext.hakuriver_mounts[@]}"; do # Iterate over array
                    HAKURIVER_CMD="${HAKURIVER_CMD} --mount '${mount}'" # Quote mounts
                done

                HAKURIVER_CMD="${HAKURIVER_CMD} -- " # The separator

                # The actual command to run inside the container is in the auto-generated __SCRIPT__ file
                # We need to execute this script *within* the container environment.
                # This requires adapting the __SCRIPT__ generation or wrapper.
                # A simpler approach might be to just put the script content directly in the submit command args,
                # or save it to a temporary file in /shared and execute that.

                # OPTION A: Simple inline command (less flexible for complex scripts)
                # cmd = "${HAKURIVER_CMD} /bin/bash -c 'your actual command ${task.ext.arg1} ${task.ext.arg2}'"

                # OPTION B: Execute auto-generated __SCRIPT__ (more complex, requires script wrapper)
                # The __SCRIPT__ is usually copied to the task's workdir on the executor.
                # We'd need to ensure the workdir is mounted and then run the script.
                # This often involves creating a small wrapper script executed by hakuriver.task submit.

                # Let's simplify for this example and assume __SCRIPT__ is copied to a known location
                # and executable, and the task workdir is accessible.
                # This requires Nextflow configuration to copy __SCRIPT__ and potentially manage workdirs.
                # If Nextflow copies the script to $PWD and $PWD is /shared/nextflow_workdir/task_id/...
                # The command might be:
                # HAKURIVER_CMD="${HAKURIVER_CMD} /bin/bash -c './.command.run'" # Execute the script Nextflow copied

                # The actual implementation of `script` for Nextflow's custom executor
                # is non-trivial and often involves more complex wrapper scripts
                # that set up the environment and run the __SCRIPT__.
                // This example is illustrative, a real implementation needs a wrapper.
                // For illustration, let's just run a dummy command that takes task.ext params:
                 cmd = "${HAKURIVER_CMD} echo 'Running process ${task.process}' with args: ${task.ext.param1} ${task.ext.param2}"
                 echo "Submitting: ${cmd}"
                 eval "${cmd}" # Execute the constructed command

            '''
        }
    }
    ```
2.  **Specify HakuRiver Options in Processes:** In your Nextflow script (`.nf`), define processes and use the `ext` block to pass HakuRiver-specific options.
    ```groovy
    process MyProcess {
        cpus 4
        memory '8 GB'
        container 'my-data-env' // Default container for this process

        // Pass HakuRiver specific options via ext
        ext {
            hakuriver_target = 'nodeA' // Specific target for this process
            // Override default container for this specific task if needed:
            // hakuriver_container = 'special-env'
            // For GPU tasks:
            // hakuriver_target = 'gpu-node' // Target node
            // hakuriver_gpus = [0, 1] // Pass GPU IDs
            // container = 'my-gpu-env' // Ensure process container is GPU-ready
            param1 = 'value1' // Example of other params your command needs
            param2 = 'value2'
        }

        input:
        path input_file

        output:
        path output_file

        script:
        """
        # This script runs INSIDE the HakuRiver container/environment
        # It's the actual task logic
        my_processor --input ${input_file} --param1 ${param1} --param2 ${param2} --output ${output_file}
        """
    }
    ```
3.  **Run Nextflow:** Execute Nextflow, ensuring the `nextflow.config` is found.
    ```bash
    # Ensure hakuriver is configured and in PATH
    nextflow run your_workflow.nf -executor hakuriver
    ```
    Nextflow will orchestrate processes, and the `hakuriver` executor will submit each process execution as a separate HakuRiver task using `hakuriver.task submit --wait`.

### Airflow

Airflow DAGs define task dependencies. You can create a custom Airflow operator or use the `BashOperator` to submit tasks to HakuRiver.

1.  **Use BashOperator:** Create an Airflow task that executes `hakuriver.task submit --wait`.
    ```python
    from airflow import DAG
    from airflow.operators.bash import BashOperator
    from datetime import datetime

    with DAG(
        dag_id='hakuriver_example_dag',
        start_date=datetime(2023, 1, 1),
        schedule=None,
        catchup=False,
    ) as dag:
        # Define a task that submits a job to HakuRiver
        process_data_task = BashOperator(
            task_id='process_data_on_hakuriver',
            bash_command=(
                "hakuriver.task submit --wait "
                "--target nodeA --cores 4 --memory 8G --container my-data-env "
                "-- " # Separator
                "python /shared/process_script.py --input /shared/data.csv --output /shared/processed.csv"
            ),
            # Set retries and retry_delay as needed for robustness
            retries=3,
            retry_delay=timedelta(minutes=5),
        )

        # Define another task dependent on the first
        train_model_task = BashOperator(
            task_id='train_model_on_hakuriver',
            bash_command=(
                "hakuriver.task submit --wait "
                "--target gpu-node::0,1 --cores 8 --memory 16G --container my-gpu-env "
                "-- " # Separator
                "python /shared/train_script.py --data /shared/processed.csv --model /shared/model.pkl"
            ),
        )

        # Define dependency
        process_data_task >> train_model_task
    ```
2.  **Ensure Execution Environment:** The Airflow worker executing this `BashOperator` must have HakuRiver installed and configured, and network access to the HakuRiver Host.

## Key Considerations for Integration

-   **`--wait` Flag:** Using `--wait` is critical for workflow managers to correctly track task completion and manage dependencies.
-   **Exit Codes:** Workflow managers rely on non-zero exit codes to detect failures. Ensure your HakuRiver tasks (both the outer `hakuriver.task submit --wait` command and the inner script/command) exit with appropriate status codes. `hakuriver.task submit --wait` should exit with a non-zero status if the submitted task fails.
-   **Environment/PATH:** Ensure the environment where the workflow manager executes the `hakuriver.task submit` command has HakuRiver in its PATH and can access the configuration file (`~/.hakuriver/config.toml`).
-   **Data Paths:** Use paths relative to the shared storage mount point (`/shared`) for input and output data within your task commands/scripts.
-   **Error Handling:** Configure retries and error handling within your workflow manager. Monitor HakuRiver logs and workflow manager logs for debugging.
-   **Resource Mapping:** Clearly map the resource requests of your workflow manager's tasks (cores, memory, containers, GPUs) to the options provided by `hakuriver.task submit`.

By using `hakuriver.task submit --wait` as the execution step within your workflow manager, you can leverage HakuRiver for its environment management and distributed execution capabilities while relying on the workflow manager for dependency orchestration, scheduling complexity, and broader pipeline management.

## Next Steps

-   [Command Task Submission Reference](../3. task-commands.md) - Details on `hakuriver.task submit` options.
-   [HakuRun Utility Guide](../7. hakurun-utility/1. utility.md) - Useful for generating task parameters *within* a workflow step.