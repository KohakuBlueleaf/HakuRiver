# HakuRun Utility Guide

The `hakurun` utility is a powerful tool that helps you run commands with multiple parameter combinations. This guide explains how to use `hakurun` both as a standalone tool and in conjunction with HakuRiver.

## What is HakuRun?

`hakurun` is a **local helper utility** with a specific purpose:
- Generate and run multiple combinations of command arguments
- Test parameter sweeps locally before submitting to the cluster
- Run combinations sequentially or in parallel
- Provide a consistent interface for parameter sweeping

It's important to understand that `hakurun` itself **does not interact with the HakuRiver cluster** - it's a local execution tool that can be used independently or combined with cluster submission.

## Argument Spanning Syntax

`hakurun` uses special syntax for expanding arguments into multiple combinations:

### Integer Ranges

Use `span:{start..end}` to generate integer sequences:

```bash
# Expands to 1, 2, 3, 4, 5
hakurun echo "Value: " span:{1..5}
```

### List Items

Use `span:[item1,item2,...]` to select from a list of items:

```bash
# Expands to "small", "medium", "large"
hakurun echo "Size: " span:[small,medium,large]
```

### Combining Spans

When multiple spans are used, `hakurun` generates the Cartesian product (all possible combinations):

```bash
# Generates all combinations of (1,2,3) Ã— (A,B) = 6 combinations
hakurun echo "Param" span:{1..3} span:[A,B]
```

This will run:
```
echo "Param" 1 A
echo "Param" 1 B
echo "Param" 2 A
echo "Param" 2 B
echo "Param" 3 A
echo "Param" 3 B
```

## Basic Usage

The general syntax is:

```bash
hakurun [--parallel] <command> <arguments...>
```

Examples:

```bash
# Run a Python script with multiple parameter combinations
hakurun python script.py span:{1..3} span:[A,B,C]

# Run a script with fixed and expanded arguments
hakurun ./process_data.py --input data.csv --output span:[output1.csv,output2.csv]
```

## Running in Parallel

By default, `hakurun` runs combinations sequentially. To run them in parallel:

```bash
hakurun --parallel python script.py span:{1..3} span:[A,B]
```

This launches all combinations concurrently as separate processes.

## Execution Targets

`hakurun` can run Python modules, functions, or executable files:

### Python Modules

Run a Python module with arguments:

```bash
hakurun python -m my_module span:{1..3}
```

### Python Functions

Run a specific function within a module:

```bash
hakurun my_module:my_function span:{1..3}
```

### Executables

Run any executable with arguments:

```bash
hakurun ./my_executable span:{1..3}
```

## Example Use Cases

### Machine Learning Hyperparameter Tuning

```bash
# Test different learning rates and batch sizes
hakurun --parallel python train_model.py --lr span:[0.001,0.01,0.1] --batch span:{32,64,128}
```

### Data Processing

```bash
# Process multiple input files with different parameters
hakurun --parallel ./process.sh span:[file1.csv,file2.csv] --threshold span:{0.1,0.2,0.3}
```

### Testing Configurations

```bash
# Test a program with different configuration options
hakurun ./test_app --mode span:[debug,release] --threads span:{1,2,4,8}
```

## HakuRun with HakuRiver

`hakurun` can be used in combination with HakuRiver in two primary ways:

### Method 1: Local Parameter Generation, Distributed Execution

Generate commands with `hakurun`, but submit each as a separate HakuRiver task:

```bash
# Generate commands with hakurun, then execute each with hakuriver.client
hakurun echo "hakuriver.client --target node1 -- python script.py" span:{1..3} span:[A,B] | bash
```

Or use a script to generate and submit tasks:

```bash
#!/bin/bash
# Submit multiple parameter combinations as separate HakuRiver tasks
for lr in 0.001 0.01 0.1; do
  for batch in 32 64 128; do
    hakuriver.client --target node1 --container ml-env -- \
      python train.py --lr $lr --batch $batch
  done
done
```

Benefits of this approach:
- Each combination runs as a separate cluster task
- Tasks can be distributed across different nodes
- Individual status tracking for each combination
- Different resource allocation for each combination if needed

### Method 2: Single Task, Local Parameter Expansion

Submit a single HakuRiver task that uses `hakurun` internally:

```bash
# Run hakurun inside a HakuRiver task
hakuriver.client --target node1 --container my-env -- \
  hakurun --parallel python script.py span:{1..3} span:[A,B]
```

Benefits of this approach:
- Simpler, single task submission
- All combinations run on one node in one container
- Less overhead (one task instead of many)

### Choosing Between Methods

Consider these factors:
- **Cluster Load**: Method 2 concentrates load on one node, Method 1 distributes it
- **Task Management**: Method 1 creates more tasks to track, Method 2 is simpler
- **Failure Handling**: With Method 1, only failed combinations need to be retried
- **Resources**: Method 2 requires a node with enough resources for all combinations

## Best Practices

### Testing Locally First

Always test your parameter sweep locally before submitting to the cluster:

```bash
# Test locally first
hakurun --parallel echo python script.py span:{1..3} span:[A,B]

# Then submit to cluster
hakuriver.client --target node1 -- hakurun --parallel python script.py span:{1..3} span:[A,B]
```

### Limiting Combinations

Be careful with the number of combinations generated:

```bash
# This creates 1,000,000 combinations!
hakurun python script.py span:{1..100} span:{1..100} span:{1..100}
```

Consider testing with a subset first:

```bash
# Test with a smaller subset
hakurun python script.py span:{1..3} span:{1..3} span:{1..3}
```

### Logging Results

Structure your scripts to log results appropriately:

```python
# In your Python script
def main(param1, param2):
    result = run_experiment(param1, param2)
    with open(f"results_{param1}_{param2}.txt", "w") as f:
        f.write(str(result))
```

### Error Handling

Ensure your scripts handle errors gracefully:

```python
# In your Python script
def main(param1, param2):
    try:
        result = run_experiment(param1, param2)
        # Save result
    except Exception as e:
        # Log error
        with open(f"error_{param1}_{param2}.txt", "w") as f:
            f.write(f"Error: {str(e)}")
```

## Limitations

- `hakurun` is a local execution tool, not a full-featured workflow system
- Parameter combinations are generated in memory, so extremely large sweeps may cause issues
- Limited error recovery and reporting compared to specialized workflow tools
- No dependency tracking between combinations

## Examples

### Basic Parameter Sweep

```bash
# File: demo_hakurun.py
import sys
print(f"Arguments received: {sys.argv[1:]}")

# Run with hakurun
hakurun python demo_hakurun.py span:{1..3} span:[A,B]
```

Output:
```
Arguments received: ['1', 'A']
Arguments received: ['1', 'B']
Arguments received: ['2', 'A']
Arguments received: ['2', 'B']
Arguments received: ['3', 'A']
Arguments received: ['3', 'B']
```

### Machine Learning Example

```bash
# File: train_model.py
import sys
import time

def main():
    learning_rate = float(sys.argv[1])
    batch_size = int(sys.argv[2])
    epochs = int(sys.argv[3])
    
    print(f"Training model with lr={learning_rate}, batch={batch_size}, epochs={epochs}")
    # Simulate training
    time.sleep(1)
    accuracy = learning_rate * batch_size * epochs / 100
    print(f"Accuracy: {min(accuracy, 0.95):.2f}")

if __name__ == "__main__":
    main()

# Run with hakurun
hakurun --parallel python train_model.py span:[0.001,0.01,0.1] span:{32,64} span:{10,20}
```

## Next Steps

- Learn about [task submission](task-submission.md) in HakuRiver
- Understand [container workflow](container-workflow.md) for consistent environments
- Explore [monitoring](monitoring.md) to track task execution