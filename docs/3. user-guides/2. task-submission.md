# Task Submission Guide

This guide explains how to submit, manage, and monitor tasks in HakuRiver, including individual tasks, batch submissions, and parameter sweeps.

## The `hakuriver.client` Command

The primary tool for submitting tasks is the `hakuriver.client` command-line utility:

```bash
hakuriver.client [options] -- [command] [arguments...]
```

The double dash (`--`) separates HakuRiver options from the command you want to run.

## Basic Task Submission

To submit a simple task to a specific node:

```bash
hakuriver.client --target node1 -- echo "Hello, HakuRiver!"
```

This will run the `echo` command on `node1` using the default container environment.

## Selecting Target Nodes

You can specify one or more target nodes for execution:

### Single Node Targeting

```bash
# Run on node1
hakuriver.client --target node1 -- python /shared/script.py
```

### Multi-Node Targeting

```bash
# Run the same task on multiple nodes
hakuriver.client --target node1 --target node2 --target node3 -- hostname
```

### NUMA Node Targeting

For NUMA-aware task placement (when using systemd fallback):

```bash
# Run on NUMA node 0 of node1
hakuriver.client --target node1:0 --container NULL -- numactl --hardware
```

## Specifying Container Environments

Select the Docker environment to run your task in:

```bash
# Use a specific container environment
hakuriver.client --target node1 --container my-python-env -- python /shared/script.py

# Run directly on the host (systemd fallback, no Docker)
hakuriver.client --target node1 --container NULL -- df -h
```

## Resource Allocation

Specify CPU and memory requirements for your tasks:

```bash
# Request 4 CPU cores
hakuriver.client --target node1 --cores 4 -- python /shared/compute_heavy.py

# Request 8GB memory
hakuriver.client --target node1 --memory 8G -- python /shared/memory_heavy.py

# Request both
hakuriver.client --target node1 --cores 4 --memory 8G -- python /shared/heavy_task.py
```

## Environment Variables

Pass environment variables to your tasks:

```bash
hakuriver.client --target node1 --env MODEL_TYPE=large --env DEBUG=1 -- python /shared/train.py
```

## Waiting for Tasks to Complete

By default, `hakuriver.client` returns immediately after submitting a task. To wait for completion:

```bash
hakuriver.client --wait --target node1 -- long_running_command
```

## Task Management

### Checking Task Status

To check the status of a specific task:

```bash
hakuriver.client --status <task_id>
```

### Viewing Task Output

View standard output and error:

```bash
# View stdout
hakuriver.client --stdout <task_id>

# View stderr
hakuriver.client --stderr <task_id>
```

### Killing Tasks

To terminate a running task:

```bash
hakuriver.client --kill <task_id>
```

### Listing Tasks

List all tasks in the system:

```bash
hakuriver.client --list-tasks
```

## Batch Task Submission with `hakurun`

HakuRiver includes a powerful utility called `hakurun` for batch parameter sweeps. This utility generates and runs multiple tasks with different parameter combinations.

### Understanding `hakurun`

`hakurun` is a local helper utility that:
1. Expands parameter combinations using special syntax
2. Executes the resulting commands either sequentially or in parallel
3. Can be used both locally and in combination with HakuRiver

### `hakurun` Argument Spanning Syntax

- `span:{start..end}` - Integer ranges
  - Example: `span:{1..5}` expands to 1, 2, 3, 4, 5
  
- `span:[item1,item2,...]` - List of items
  - Example: `span:[small,medium,large]` expands to "small", "medium", "large"

### Running Parameter Sweeps Locally

```bash
# Run combinations locally and sequentially
hakurun python script.py span:{1..3} span:[A,B]

# Run combinations locally in parallel
hakurun --parallel python script.py span:{1..3} span:[A,B]
```

This will run 6 combinations: (1,A), (1,B), (2,A), (2,B), (3,A), (3,B)

### Combining `hakurun` with HakuRiver (Method 1)

Use `hakurun` locally, but submit each combination as a separate HakuRiver task:

```bash
# Generate all combinations and submit each as a separate task
for i in {1..3}; do
  for param in A B; do
    hakuriver.client --target node1 --container my-env -- python script.py $i $param
  done
done
```

Or use `hakurun` to generate the commands:

```bash
# Use hakurun to generate commands, then pipe to bash
hakurun echo hakuriver.client --target node1 -- python script.py span:{1..3} span:[A,B] | bash
```

This approach creates separate HakuRiver tasks for each parameter combination, allowing:
- Different nodes to handle different combinations
- Independent tracking of each combination
- Fine-grained resource allocation per combination

### Combining `hakurun` with HakuRiver (Method 2)

Submit a single HakuRiver task that runs `hakurun` internally:

```bash
# Run hakurun inside a HakuRiver task
hakuriver.client --target node1 --container my-env -- hakurun --parallel python script.py span:{1..3} span:[A,B]
```

This approach:
- Creates only one HakuRiver task
- Runs all combinations on a single node
- Uses only one containerized environment
- Parallelizes within the node (if `--parallel` is used)

### Choosing Between Methods

- **Method 1**: Better for distributing combinations across nodes, independent tracking
- **Method 2**: Simpler, less overhead, but all combinations run on one node

## Advanced Task Submission Techniques

### Running With Privileged Mode

For tasks requiring elevated privileges:

```bash
hakuriver.client --target node1 --privileged -- docker info
```

### Mounting Additional Directories

To mount additional directories into the container:

```bash
hakuriver.client --target node1 --mount /data:/container/data -- ls -la /container/data
```

### Task Submission with Custom Configuration

Specify a custom configuration file:

```bash
hakuriver.client --config custom-config.toml --target node1 -- command
```

## Examples

### Running a Simple Data Processing Task

```bash
hakuriver.client --target node1 --container python-data --cores 2 -- \
  python /shared/process_data.py --input /shared/data.csv --output /shared/results.csv
```

### Running a Parameter Sweep for ML Model Training

```bash
# Method 1: Each combination as separate task
for learning_rate in 0.001 0.01 0.1; do
  for hidden_size in 64 128 256; do
    hakuriver.client --target node1 --container ml-env --cores 4 --memory 8G -- \
      python /shared/train_model.py --lr $learning_rate --hidden $hidden_size
  done
done

# Method 2: Using hakurun inside one task
hakuriver.client --target gpu-node --container ml-env --cores 8 --memory 16G -- \
  hakurun --parallel python /shared/train_model.py \
  span:[0.001,0.01,0.1] span:[64,128,256]
```

### Data Processing on Multiple Nodes

```bash
# Process different datasets on different nodes
hakuriver.client --target node1 --container data-proc -- python /shared/process.py --dataset A
hakuriver.client --target node2 --container data-proc -- python /shared/process.py --dataset B
hakuriver.client --target node3 --container data-proc -- python /shared/process.py --dataset C
```

### Running a Multi-Stage Pipeline

```bash
# Stage 1: Data preparation
TASK1=$(hakuriver.client --wait --target node1 --container data-proc -- \
  python /shared/prepare_data.py)
echo "Data preparation completed with task ID: $TASK1"

# Stage 2: Training (only if Stage 1 succeeded)
if [ "$?" -eq 0 ]; then
  TASK2=$(hakuriver.client --wait --target gpu-node --container ml-env -- \
    python /shared/train_model.py)
  echo "Training completed with task ID: $TASK2"
fi
```

## Best Practices

### Resource Allocation

- Specify appropriate `--cores` and `--memory` for your tasks
- Avoid overcommitting resources on nodes
- For memory-intensive tasks, set a realistic limit to avoid OOM kills

### Task Organization

- Use consistent naming and parameters for related tasks
- Consider batching related tasks together
- Use shared storage for inputs/outputs

### Error Handling

- Check task status and outputs after submission
- Implement retry logic for failed tasks
- Ensure your scripts handle errors gracefully

## Next Steps

- Learn about [monitoring tasks](monitoring.md)
- Understand [resource allocation](resource-allocation.md) in detail
- Explore the [web dashboard](web-dashboard.md) for visual task management