# HakuRun Utility Guide

The `hakurun` utility is a powerful tool that helps you run commands with multiple parameter combinations. This guide explains how to use `hakurun` both as a standalone tool and in conjunction with HakuRiver.

## What is HakuRun?

`hakurun` is a **local helper utility** with a specific purpose:
-   Generate and run multiple combinations of command arguments based on defined "spanning" syntax.
-   Test parameter sweeps or batch job commands locally before submitting them to a distributed system like HakuRiver.
-   Run the generated command combinations either sequentially or in parallel on your local machine.
-   Provide a consistent interface for simple parameter sweeping.

It's important to understand that `hakurun` itself **does not interact with the HakuRiver cluster** - it's a local execution tool. It's typically used on a client machine to prepare or test commands that will later be submitted to HakuRiver.

## Argument Spanning Syntax

`hakurun` uses special syntax directly within command arguments to define sets of values. It then generates all combinations by taking the Cartesian product of these sets.

### Integer Ranges: `span:{start..end}`

Generates a sequence of integers from `start` to `end` (inclusive).

```bash
# Expands to 1, 2, 3, 4, 5
hakurun echo "Value: " span:{1..5}
```
Output:
```
Value: 1
Value: 2
Value: 3
Value: 4
Value: 5
```

### List Items: `span:[item1,item2,...]`

Generates values from a comma-separated list of items. Items can contain spaces if quoted or escaped, but commas within items must be handled carefully (e.g., by putting the whole item in quotes like `"item with, comma"` or escaping the comma).

```bash
# Expands to "small", "medium", "large"
hakurun echo "Size: " span:[small,medium,large]
```
Output:
```
Size: small
Size: medium
Size: large
```
Note that the items are treated as strings. If you need numbers, use integer ranges or parse the string argument in your script.

### Combining Spans

When multiple spanning arguments are used, `hakurun` generates all possible combinations by taking the Cartesian product:

```bash
# Generates all combinations of (1,2,3) Ã— (A,B) = 6 combinations
hakurun echo "Param" span:{1..3} span:[A,B]
```
This will run the `echo "Param" <value_from_range> <value_from_list>` command 6 times:
```
Param 1 A
Param 1 B
Param 2 A
Param 2 B
Param 3 A
Param 3 B
```
The order of execution might vary, especially with `--parallel`.

## Basic Usage

The general syntax is:

```bash
hakurun [--parallel] <app> [app_arguments...]
```

-   `--parallel`: Optional flag to run generated commands concurrently using local subprocesses.
-   `<app>`: The target to run. This can be a standard executable name (e.g., `python`), a path to an executable script (e.g., `./my_script.sh`), or a Python module/function reference (e.g., `my_module:my_function` or `my_module`).
-   `[app_arguments...]`: Arguments to pass to the target application. This is where you include the `span:` syntax.

Examples:

```bash
# Run a Python script with multiple parameter combinations sequentially
hakurun python script.py span:{1..3} span:[A,B,C]

# Run a shell script in parallel with different input files
hakurun --parallel ./process_data.sh --input span:[data_part_1.csv,data_part_2.csv] --output-prefix results_
```

## Execution Targets (`<app>`)

The `<app>` argument specifies what `hakurun` should execute for each argument combination.

-   **Executable Name or Path**: If `<app>` looks like an executable name or path (doesn't contain `:`), `hakurun` will run it directly using `subprocess.Popen`.
    ```bash
    hakurun ./my_program --iterations span:{100..200}
    hakurun python my_script.py --alpha span:[0.1,0.01]
    ```
-   **Python Module**: If `<app>` is in `<module>` format, `hakurun` will import the module. This is useful for modules designed to run directly when imported or used with `-m`.
    ```bash
    hakurun my_module span:{1..5} # Equivalent to `python -m my_module 1`, `python -m my_module 2`, etc.
    ```
-   **Python Function**: If `<app>` is in `<module>:<function>` format, `hakurun` will import the specified module and call the specified function with the generated arguments unpacked.
    ```python
    # my_module.py
    def process(input_id, output_label):
        print(f"Processing ID {input_id} with label {output_label}")

    # Command:
    # hakurun my_module:process span:{1..2} span:[A,B]
    ```
    This would call `my_module.process(1, 'A')`, `my_module.process(1, 'B')`, etc. The arguments passed to the function will be the actual values (integers, strings) from the spanning, not necessarily strings like in `sys.argv`.

## Running in Parallel (`--parallel`)

By default, `hakurun` runs each generated command combination sequentially, waiting for one to finish before starting the next. The `--parallel` flag runs combinations concurrently using multiple subprocesses. This can significantly speed up sweeps on multi-core local machines.

```bash
# Run 100 simulations concurrently if your machine has enough cores
hakurun --parallel ./simulate --seed span:{1..100}
```
The degree of parallelism is typically limited by the number of CPU cores and system resources.

## Example Use Cases

-   **Machine Learning Hyperparameter Tuning**: Test different combinations of learning rates, batch sizes, optimizer types, etc.
    ```bash
    hakurun --parallel python train.py --lr span:[0.001,0.01,0.1] --batch-size span:[32,64,128] --optimizer span:[adam,sgd]
    ```
-   **Data Processing**: Process different input files with various parameter settings.
    ```bash
    hakurun --parallel ./process_file --input-file span:[file1.csv,file2.csv,file3.csv] --mode span:[fast,precise] --output-dir /results
    ```
-   **Testing Configuration Combinations**: Test an application or script with different configuration flags or values.
    ```bash
    hakurun --parallel ./test_app --featureA span:[true,false] --level span:{1..5} --output-format span:[json,yaml]
    ```

## HakuRun with HakuRiver

`hakurun` is a local tool, but it can be used effectively alongside `hakuriver.task submit` in two main ways:

### Method 1: Local Parameter Generation, Distributed Execution

Use `hakurun` on your client machine to generate all the specific command lines needed for your sweep, and then execute each generated line as a separate `hakuriver.task submit` command. This creates one HakuRiver task for *each* parameter combination.

```bash
# Example: Submit a parameter sweep to nodeA, with each combination as a separate HakuRiver task.
# This uses bash process substitution or piping.
hakurun echo "hakuriver.task submit --target nodeA --container my-ml-env -- python /shared/train.py" span:[0.001,0.01] span:[64,128] | bash
# Output of hakurun echo ... will be:
# hakuriver.task submit --target nodeA --container my-ml-env -- python /shared/train.py 0.001 64
# hakuriver.task submit --target nodeA --container my-ml-env -- python /shared/train.py 0.001 128
# hakuriver.task submit --target nodeA --container my-ml-env -- python /shared/train.py 0.01 64
# hakuriver.task submit --target nodeA --container my-ml-env -- python /shared/train.py 0.01 128
# The pipe (| bash) then executes each line as a command.

# Alternative scripting approach (e.g., in a .sh script)
# for lr in 0.001 0.01; do
#   for bs in 64 128; do
#     hakuriver.task submit --target nodeA --container my-ml-env -- python /shared/train.py --lr $lr --batch-size $bs
#   done
# done
```
Benefits of this method:
-   Each parameter combination is an independent task in HakuRiver, allowing individual monitoring, logging, killing, and retrying.
-   HakuRiver can distribute these independent tasks across multiple available Runner nodes.
-   Resource requests (`--cores`, `--memory`, etc.) are applied per combination/task.

### Method 2: Single Task, Local Parameter Expansion (within the task)

Submit a *single* HakuRiver task where the command to be executed on the Runner is `hakurun`, followed by the application and spanning arguments. `hakurun` will then run the entire parameter sweep *inside* that single task's environment on the assigned Runner.

```bash
# Example: Run the entire parameter sweep inside one task on nodeB using GPUs 0,1.
hakuriver.task submit --target nodeB::0,1 --cores 8 --memory 32G --container my-gpu-env -- \
  hakurun --parallel python /shared/train.py span:[0.001,0.01] span:[64,128]
```
Benefits of this method:
-   Simpler HakuRiver task management (one task entry instead of many).
-   All combinations share the same container environment and allocated resources on a single node.
-   Parallelism (`--parallel`) happens within the single assigned Runner node.

### Choosing Between Methods

Consider these factors:
-   **HakuRiver Task Management**: Method 1 creates many tasks; Method 2 creates one. Choose Method 2 if you prefer less clutter in the HakuRiver task list and monitoring.
-   **Distribution**: Method 1 can distribute combinations across different nodes; Method 2 runs all on one node. Choose Method 1 if your sweep is large and you have multiple nodes available, or if intermediate results of one combination shouldn't interfere with others by sharing the same `/local_temp` or process space on the Runner.
-   **Fault Tolerance**: If one combination fails in Method 1, only that specific task is marked failed; others continue. In Method 2, if any combination causes `hakurun` or the inner command/script to exit with a non-zero status, the entire HakuRiver task is marked failed.
-   **Resource Requirements**: Method 1 applies resource limits per combination. Method 2 applies limits to the single task running `hakurun` itself, which must be sufficient for *all* parallel subprocesses started by `hakurun --parallel`.

## Best Practices when using HakuRun

-   **Test Locally First**: Always run your `hakurun` command without submitting to HakuRiver first (`hakurun ...`) to ensure the syntax is correct and it generates the expected commands/function calls.
-   **Limit Combinations**: Be mindful of the number of combinations the Cartesian product generates, especially with multiple spanning arguments. `hakurun` can quickly create a huge number of tasks/commands.
-   **Script Robustness**: Design the script or function being called by `hakurun` to handle its specific arguments and perform necessary actions like logging results or handling errors within its own execution context. Outputting to files within `/shared` (mapped from `shared_dir/shared_data`) is a good pattern.
-   **Quoting**: Use quotes around spanning arguments and the target command/arguments, especially if they contain spaces or special characters, to ensure they are parsed correctly by `hakurun` and the shell.

## Limitations

-   `hakurun` is a simple argument expansion tool, not a full workflow engine like Airflow or Nextflow. It doesn't manage dependencies *between* generated tasks/combinations.
-   Argument combinations are generated in memory, which could become an issue for extremely large sweeps.

## Examples

A simple example using `demo_hakurun.py` (from the project files):

```python
# demo_hakurun.py
import sys
import time
import random
import os # Import os to get PID

time.sleep(random.random() * 0.1) # Simulate work
# Print arguments received from hakurun (sys.argv[0] is the script name)
print(f"Args: {sys.argv[1:]}, PID: {os.getpid()}")
```

Running locally with `hakurun`:

```bash
# Run 2 * 1 * 2 = 4 tasks locally and in parallel
hakurun --parallel python ./demo_hakurun.py span:{1..2} fixed_arg span:[input_a,input_b]
```
Example output (order and PIDs vary):
```
[HakuRun]-|..|-INFO: Running 4 tasks in parallel via subprocess...
[HakuRun]-|..|-INFO:   Task 1/4: python ./demo_hakurun.py 1 fixed_arg input_a
[HakuRun]-|..|-INFO:   Task 2/4: python ./demo_hakurun.py 1 fixed_arg input_b
[HakuRun]-|..|-INFO:   Task 3/4: python ./demo_hakurun.py 2 fixed_arg input_a
[HakuRun]-|..|-INFO:   Task 4/4: python ./demo_hakurun.py 2 fixed_arg input_b
Args: ['1', 'fixed_arg', 'input_b'], PID: 12345
Args: ['2', 'fixed_arg', 'input_a'], PID: 12346
Args: ['1', 'fixed_arg', 'input_a'], PID: 12347
Args: ['2', 'fixed_arg', 'input_b'], PID: 12348
[HakuRun]-|..|-INFO: Waiting for parallel tasks to complete...
[HakuRun]-|..|-INFO: All parallel tasks finished successfully.
```

Using this with HakuRiver (Method 1 - separate tasks):

```bash
# On a Client machine
hakurun echo "hakuriver.task submit --target node1 --container my-py-env -- python /shared/demo_hakurun.py" span:{1..2} fixed_arg span:[input_a,input_b] | bash
```
This would submit 4 separate tasks to HakuRiver, each running the `demo_hakurun.py` with one argument combination on `node1`.

Using this with HakuRiver (Method 2 - single task):

```bash
# On a Client machine
hakuriver.task submit --target node1 --cores 4 --container my-py-env -- \
  hakurun --parallel python /shared/demo_hakurun.py span:{1..2} fixed_arg span:[input_a,input_b]
```
This would submit 1 task to HakuRiver. That task, running on `node1` in `my-py-env` with 4 cores allocated to the *single* container/scope, would then execute `hakurun` which in turn launches 4 Python subprocesses inside that container/scope.

## Next Steps

-   Learn more about [Command Task Submission](../2. command-tasks/1. submission.md).
-   Understand the [Docker Container Workflow](../1. container-workflow.md) for preparing the environments used by your tasks.
-   Explore [Monitoring](../6. monitoring/1. monitoring.md) options to track the execution and output of your tasks.