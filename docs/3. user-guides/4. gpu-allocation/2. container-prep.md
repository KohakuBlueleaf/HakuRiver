# Preparing Containers for GPU Tasks

GPU tasks require Docker images with NVIDIA CUDA libraries installed. This guide shows how to prepare GPU-enabled container environments.

---

## Requirements

For a container to use GPUs:

| Requirement | Description |
|-------------|-------------|
| CUDA Toolkit | Core NVIDIA GPU libraries |
| cuDNN | Deep learning operations (optional) |
| Framework | PyTorch, TensorFlow, etc. |
| Driver compatibility | CUDA version must match Runner driver |

---

## Quick Setup

### Option 1: Start from NVIDIA Base Image

Easiest approach - use official NVIDIA images:

```bash
# CUDA runtime
kohakuriver docker container create nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04 cuda-env

# TensorFlow GPU
kohakuriver docker container create tensorflow/tensorflow:latest-gpu tf-gpu-env

# PyTorch (NGC)
kohakuriver docker container create nvcr.io/nvidia/pytorch:23.10-py3 pytorch-env
```

### Option 2: Add to Existing Container

```bash
# Start from your base
kohakuriver docker container create ubuntu:22.04 my-gpu-base

# Install CUDA manually (complex - prefer Option 1)
kohakuriver docker container shell my-gpu-base
# Follow NVIDIA CUDA installation docs...
```

---

## Step-by-Step: PyTorch GPU Environment

### 1. Create Container

```bash
kohakuriver docker container create nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04 pytorch-cuda
```

### 2. Install PyTorch

```bash
kohakuriver docker container shell pytorch-cuda
```

Inside container:

```bash
# Update and install basics
apt-get update
apt-get install -y python3 python3-pip git

# Install PyTorch with CUDA 11.8
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Install other packages
pip3 install numpy pandas matplotlib

# Cleanup
apt-get clean
rm -rf /var/lib/apt/lists/*

exit
```

### 3. Package

```bash
kohakuriver docker tar create pytorch-cuda
```

### 4. Test

```bash
kohakuriver task submit "python3 -c 'import torch; print(torch.cuda.is_available())'" \
  -t node1::0 --container pytorch-cuda
```

---

## Step-by-Step: TensorFlow GPU Environment

### 1. Create from TensorFlow Image

```bash
kohakuriver docker container create tensorflow/tensorflow:2.14.0-gpu tf-cuda
```

### 2. Add Additional Packages

```bash
kohakuriver docker container shell tf-cuda
```

Inside:

```bash
pip install keras scikit-learn pandas matplotlib

apt-get clean
exit
```

### 3. Package

```bash
kohakuriver docker tar create tf-cuda
```

---

## CUDA Version Compatibility

### Check Runner Driver Version

On Runner node:

```bash
nvidia-smi
# Look for "Driver Version" and "CUDA Version"
```

### Compatibility Matrix

| Driver Version | Max CUDA |
|----------------|----------|
| 450.x | 11.0 |
| 470.x | 11.4 |
| 510.x | 11.6 |
| 520.x | 11.8 |
| 525.x | 12.0 |
| 535.x | 12.2 |

**Rule:** Container CUDA version must be ≤ Driver CUDA version.

Example: Driver shows CUDA 12.0 → Use CUDA 11.8 or earlier in container.

---

## Common Base Images

### NVIDIA CUDA Images

| Image | Use Case |
|-------|----------|
| `nvidia/cuda:11.8.0-base-ubuntu22.04` | Minimal CUDA |
| `nvidia/cuda:11.8.0-runtime-ubuntu22.04` | Runtime libraries |
| `nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04` | Runtime + cuDNN |
| `nvidia/cuda:11.8.0-devel-ubuntu22.04` | Development tools |

### Framework Images

| Image | Framework |
|-------|-----------|
| `tensorflow/tensorflow:2.14.0-gpu` | TensorFlow 2.14 |
| `pytorch/pytorch:2.0.1-cuda11.8-cudnn8-runtime` | PyTorch 2.0 |
| `nvcr.io/nvidia/pytorch:23.10-py3` | NGC PyTorch |

---

## Troubleshooting

### Cannot Pull NVIDIA Images

```bash
# On Host: ensure Docker can reach registry
docker pull nvidia/cuda:11.8.0-base-ubuntu22.04
```

If blocked:
- Check network/proxy settings
- Try NGC registry (`nvcr.io/nvidia/...`)
- Pull on Runner, save, transfer to Host

### CUDA Not Working in Task

| Symptom | Solution |
|---------|----------|
| `libcuda.so not found` | Use runtime/devel image, not base |
| `CUDA version mismatch` | Check driver compatibility |
| `torch.cuda.is_available() = False` | Verify PyTorch CUDA version matches |

### Debug Inside Container

```bash
# Test GPU visibility
kohakuriver task submit "nvidia-smi" -t node1::0 --container cuda-env

# Test CUDA runtime
kohakuriver task submit "python3 -c 'import torch; print(torch.version.cuda)'" \
  -t node1::0 --container pytorch-cuda
```

---

## Best Practices

| Practice | Description |
|----------|-------------|
| **Use official images** | Pre-built NVIDIA/framework images |
| **Match CUDA to driver** | Check compatibility before building |
| **Use runtime images** | Smaller than devel for production |
| **Test before packaging** | Verify GPU access works |
| **Clean caches** | Reduce image size |

### Image Size Tips

```bash
# Inside container before packaging:
apt-get clean
rm -rf /var/lib/apt/lists/*
pip cache purge
```

---

## Example: Complete ML Environment

```bash
# 1. Create from CUDA base
kohakuriver docker container create nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04 ml-env

# 2. Install everything
kohakuriver docker container shell ml-env
```

Inside:

```bash
apt-get update
apt-get install -y python3 python3-pip git wget

# PyTorch with CUDA
pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# ML ecosystem
pip3 install \
    numpy pandas scipy scikit-learn \
    matplotlib seaborn \
    jupyter ipython \
    transformers datasets \
    wandb tensorboard

# Cleanup
apt-get clean
rm -rf /var/lib/apt/lists/*
pip cache purge

exit
```

Package:

```bash
kohakuriver docker tar create ml-env
```

---

## Next Steps

1. [GPU allocation guide](1.%20allocation.md)
2. [Command task submission](../2.%20command-tasks/1.%20submission.md)
3. [VPS with GPU access](../3.%20vps-tasks/1.%20management.md)
