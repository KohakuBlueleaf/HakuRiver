# Preparing Your Container Image for GPU Tasks

To run tasks that utilize GPUs with HakuRiver, the Docker container image used by the task must be properly prepared with the necessary NVIDIA libraries and tools. This guide outlines the key steps using the [Container Workflow Guide](../1. container-workflow.md).

## Essential Requirements for a GPU Container

A Docker image intended for GPU tasks *must* contain:

1.  The **NVIDIA CUDA Toolkit** (including libraries like `libcuda.so`, `libcublas.so`, `libcudnn.so`).
2.  Compatible versions of **cuDNN** if you are doing deep learning.
3.  Potentially other GPU-accelerated libraries (TensorFlow, PyTorch, etc.).

These libraries inside the container must be compatible with the **NVIDIA driver version installed on the Runner node**. The NVIDIA Container Toolkit on the Runner handles making the host GPU devices accessible to the container, but the container needs the user-space libraries to interface with them.

## Step-by-Step Preparation

Use the standard HakuRiver container workflow to prepare your GPU image.

**Phase 1: Create and Customize on the Host**

1.  **Choose a Base Image**: The easiest and most reliable way is to start with an **official NVIDIA CUDA base image** or a deep learning framework image provided by NVIDIA or the framework developers (e.g., TensorFlow, PyTorch). These images come with the CUDA toolkit, cuDNN, and other base libraries pre-installed and configured.
    - Select an image tag that matches or is compatible with the NVIDIA driver version on your Runner nodes. Check the CUDA Toolkit compatibility matrix with NVIDIA drivers.
    ```bash
    # On your Client machine (connected to Host)

    # Example: Use a CUDA runtime image
    hakuriver.docker create-container nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04 my-cuda-env-base

    # Example: Use a TensorFlow GPU image
    # hakuriver.docker create-container tensorflow/tensorflow:latest-gpu my-tf-gpu-env-base
    ```
    Make sure the Host machine can pull this base image.

2.  **Install Task-Specific Software**: Get an interactive shell into the base container on the Host and install the specific software, libraries, and dependencies required by your task script (e.g., a specific version of PyTorch, scikit-learn, custom code).
    ```bash
    hakuriver.docker-shell my-cuda-env-base

    # Inside the container shell:
    # apt-get update # Or yum/apk depending on base image
    # Example: Install PyTorch with specific CUDA version compatibility
    # pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
    #
    # Example: Install other necessary packages
    # apt-get install -y git wget build-essential

    # Clean up package manager cache to reduce image size
    # apt-get clean && rm -rf /var/lib/apt/lists/* # For Debian/Ubuntu

    # Exit the shell
    exit
    ```
    Ensure all dependencies your script needs are installed here.

**Phase 2: Package and Distribute**

3.  **Create the Tarball**: Once the persistent Host container contains all necessary GPU libraries and task software, package its state into a distributable tarball.
    ```bash
    # On your Client machine
    hakuriver.docker create-tar my-cuda-env-base
    ```
    This commits the changes and saves the image `hakuriver/my-cuda-env-base:base` to a versioned tarball in your shared storage.

4.  **Verify Tarball**: Confirm the tarball exists and is listed.
    ```bash
    hakuriver.docker list-tars
    ```

## Using the Prepared Container for GPU Tasks

When you submit a Command or VPS task with `--container <your_gpu_env_name>` and specify GPUs using the `::GPU_IDs` syntax (e.g., `hakuriver.task submit --target nodeA::0,1 --container my-cuda-env-base ...`), the Runner will:

1.  Sync the container image if needed.
2.  Launch the Docker container.
3.  Pass the `--gpus device=<id1,id2,...>` flag to `docker run`.
4.  Because your image contains the necessary CUDA/cuDNN libraries and the Runner has the NVIDIA Container Toolkit, the task running inside the container will be able to access and utilize the specified GPUs.

## Troubleshooting Container Preparation for GPUs

-   **`hakuriver.docker create-container` fails for NVIDIA images**: Ensure your Host machine's Docker installation is set up to pull from the public Docker registry and has internet access. The Host doesn't necessarily need NVIDIA drivers itself, but it needs network access to pull the base image.
-   **Errors during `apt-get install` or `pip install` inside the container shell**: Standard package management issues. Check network connectivity from the container (DNS, internet), verify repository sources, and check for dependency conflicts.
-   **Errors when trying to run GPU code *after* creating the tarball**: This indicates the image was created successfully, but the setup *inside* the container might be wrong, or there's a compatibility issue with the Runner node. Debug by:
    -   Launching a *temporary* container manually on the **Runner node** using the `hakuriver/<your_gpu_env_name>:base` image (which the Runner synced): `docker run --rm --gpus all hakuriver/<your_gpu_env_name>:base nvidia-smi`. Does `nvidia-smi` work inside this container?
    -   Manually running your task script inside this temporary container on the Runner.
    -   Verify compatibility between the libraries installed in the image and the Runner's NVIDIA driver version.

## Next Steps

-   Learn how to [Request GPUs for Your Tasks](../1. allocation.md).
-   Understand how to [Monitor GPU Resources and Utilization](../5. web-dashboard/1. overview.md).
-   Review the [Troubleshooting Guide](../5. troubleshooting/1. common-issues.md) for common GPU-related errors.