# GPU Allocation Guide

This guide explains how HakuRiver handles GPU resources and how you can request specific GPUs for your tasks using the `--target` flag. GPU allocation is currently only supported for tasks running inside **Docker containers**.

## How HakuRiver Manages GPUs

1.  **Runner Reporting**: When a Runner agent starts and sends heartbeats, it detects available NVIDIA GPUs (using `pynvml`) and reports their details (model name, total memory, utilization, temperature, etc.) and unique integer IDs (0, 1, 2, ...) to the Host.
2.  **Host Tracking**: The Host stores the GPU information reported by each online Runner in its database. It uses this information to track available GPUs and validate task requests.
3.  **Client/Dashboard View**: You can view the reported GPU status and utilization for all online nodes via the `hakuriver.client health` command or the dedicated "GPUs" view in the Web Dashboard.
4.  **Task Allocation**: When you submit a Command task or a VPS task with requested GPUs, the Host checks if the targeted node is online, has the specified GPU IDs available (not currently allocated to other running/pending tasks), and then assigns the task.
5.  **Runner Execution**: The Runner receives the task request including the list of required GPU IDs. When launching the Docker container for the task, it passes these IDs to Docker using the `--gpus device=<id1,id2,...>` flag. Docker then ensures only those specific GPUs are accessible within the container.

## Requesting GPUs for Tasks

You request specific GPUs as part of the `--target` specification when submitting a task (Command or VPS).

### Target Syntax with GPUs

The GPU part of the target syntax is `::GPU_ID1[,GPU_ID2,...]`. It's appended to the hostname.

-   `--target node1::0`: Request GPU with ID 0 on `node1`.
-   `--target nodeA::0,1,3`: Request GPUs with IDs 0, 1, and 3 on `nodeA`.

You can combine this with a hostname only, but not with NUMA ID targeting (`:NUMA_ID`). The format is `HOST[::GPU_IDS]`.

**Important Notes:**
-   GPU allocation using this syntax is only supported for tasks running inside **Docker containers**.
-   It requires the Runner node to have compatible NVIDIA drivers and the NVIDIA Container Toolkit installed and configured for Docker.
-   The Docker image used by the task must also contain the necessary CUDA/GPU libraries compatible with the driver version on the Runner.

### Examples:

**Command Task Submission (`hakuriver.task submit`)**

-   Run a script on `node1` using GPU 0:
    ```bash
    hakuriver.task submit --target node1::0 --container my-cuda-env -- python /shared/train_gpu.py --device 0
    ```
-   Run a task on `nodeB` using GPUs 2 and 3:
    ```bash
    hakuriver.task submit --target nodeB::2,3 --container my-gpu-env -- python /shared/multi_gpu_job.py
    ```
-   **Multi-Target GPU Tasks:** Submit the same task to multiple nodes, each getting specified GPUs (creates one task instance per target):
    ```bash
    hakuriver.task submit --target nodeC::0 --target nodeD::1 --container another-gpu-env -- python /shared/single_gpu_script.py
    ```

**VPS Task Submission (`hakuriver.vps submit`)**

-   Launch a VPS on `nodeE` using GPU 0:
    ```bash
    hakuriver.vps submit --target nodeE::0 --container my-cuda-vps --public-key-file ~/.ssh/id_rsa.pub --cores 0 --memory 8G
    ```
-   Launch a VPS on `nodeF` using GPUs 0, 1, and 2:
    ```bash
    hakuriver.vps submit --target nodeF::0,1,2 --container my-gpu-vps --public-key-string "ssh-rsa AAAA..." --cores 4
    ```
-   **Auto-select node with GPUs:** Omit `--target` and only specify required cores/memory/GPUs (not supported directly by `--target` syntax, but Host logic *might* support auto-selection based on GPU *requests* in the payload if `--target` is omitted in the API call - check API docs/Web UI form for confirmation). The `hakuriver.vps` CLI requires `--target` currently, but the Web UI allows auto-select with GPU requests.

## Checking Available GPUs

Before submitting tasks, you can check which GPUs are available and their status on each node:

-   **CLI:** Use the `hakuriver.client health` command. It includes a section for GPU information reported by each online node.
    ```bash
    hakuriver.client health # See GPUs for all nodes
    hakuriver.client health nodeA # See GPUs for a specific node
    ```
-   **Web Dashboard:** The "GPUs" view provides a clear table of GPUs available on each node, including their utilization, temperature, and other details.

When submitting a task via the Web Dashboard, the forms will often show available nodes and GPUs, simplifying selection and avoiding targeting resources that are offline or don't exist.

## Monitoring GPU Tasks

Once a task using GPUs is running, its status and resource usage can be monitored:

-   **CLI:** `hakuriver.client status <task_id>` will show the assigned node and the `required_gpus` that were allocated to the task.
-   **Web Dashboard:** The "Tasks" or "VPS" views will show the assigned node and requested GPUs. The "GPUs" view will show the overall utilization of each GPU on each node, allowing you to see if your task is actively using the allocated GPUs.

## Container Environment Setup for GPUs

For GPU allocation to work, the Docker image used by the task (`--container <env_name>`) must be prepared with the necessary libraries.

-   Use a base image from NVIDIA (e.g., `nvidia/cuda`, `tensorflow/tensorflow:*-gpu`).
-   Ensure the CUDA toolkit, cuDNN, and other required libraries are installed inside the container and are compatible with the NVIDIA driver version on the Runner nodes.
-   Prepare this environment on the Host using `hakuriver.docker create-container` and `hakuriver.docker-shell`, then package it with `hakuriver.docker create-tar`.

See the [Container Workflow Guide](../1. container-workflow.md) for details on preparing container environments.

## Troubleshooting GPU Tasks

-   **Task Failed (container exit code):** If a GPU task fails immediately, check the logs (`hakuriver.task stdout/stderr`) and Runner systemd logs (`journalctl -u hakuriver-runner.service`). Common issues include:
    -   NVIDIA Container Toolkit not installed or misconfigured on the Runner node.
    -   GPU IDs specified are incorrect or not available on the target node.
    -   The Docker image doesn't have necessary CUDA/GPU libraries, or they are incompatible with the driver version on the Runner.
    -   The task script itself requires specifying the GPU ID, and the script's method doesn't match the allocated ID(s).
-   **GPU Utilization is 0%**: If a task is running but GPU utilization remains 0%, it suggests the task is not correctly using the available GPU resources within the container. This is often an issue with the task's code or the container environment setup, not HakuRiver itself. Verify the container environment has the GPU libraries and your task code is correctly configured to use the visible devices.
-   **Error finding GPU info on Runner:** If `hakuriver.client health <node>` shows no GPU info or errors, check if `pynvml` is installed and the NVIDIA driver is correctly loaded on that Runner.

## Next Steps

-   Learn more about [Command Task Submission](../2. command-tasks/1. submission.md).
-   Learn more about [VPS Task Management](../3. vps-tasks/1. management.md).
-   Prepare your [Docker Container Environments](../1. container-workflow.md) for GPU tasks.
-   Use the [Web Dashboard](../5. web-dashboard/1. overview.md) for a visual overview of GPU resources.