# Command Task Best Practices

This guide provides tips and recommended patterns for designing and writing the command-line tasks and scripts that you submit to HakuRiver using `hakuriver.task submit`. Following these practices can improve task reliability, debugging, and resource utilization.

## Designing Your Command or Script

The core of a HakuRiver Command task is a single command or script that is executed.

1.  **Keep it Focused**: Design your task to perform a single, well-defined job (e.g., process one file, run one simulation iteration, train one model epoch). This makes tasks easier to manage, track, and parallelize.
2.  **Use Command-Line Arguments**: Pass parameters to your script or command using command-line arguments (e.g., `--input-file path/to/data`, `--iterations 100`). Avoid hardcoding paths or parameters inside the script itself. This allows you to use `hakurun` for local testing and makes tasks flexible for different inputs or configurations when submitting to HakuRiver.
3.  **Standard Output and Error**: Write progress updates and results to standard output (`stdout`) and error messages, warnings, or debugging info to standard error (`stderr`). HakuRiver captures these streams into separate files in `shared_dir` (`.out` and `.err` files) and makes them accessible via `hakuriver.task stdout/stderr` and the Web Dashboard.
4.  **Exit Codes**: Your script should exit with a status code of `0` for success and a non-zero code (conventionally 1) for failure. HakuRiver uses the exit code to determine the task's final status (`completed` or `failed`).
5.  **Idempotency (where possible)**: Design tasks to be idempotent if feasible, meaning running the same task with the same inputs multiple times has the same effect as running it once. This simplifies retries if a task fails partway through.

## Leveraging Shared Storage

The `shared_dir` configured on Host and Runners is typically mounted into Docker containers as `/shared`. This is the primary location for task inputs, outputs, and scripts.

-   **Input Data**: Place input files or directories within `shared_dir/shared_data/` (or another location mounted into the container). Reference these paths using their *container path* (e.g., `/shared/data/input.csv`).
-   **Output Data**: Write task results and output files to directories within `shared_dir/task_outputs/` or `shared_dir/shared_data/`. Use absolute paths relative to the container's mount point (e.g., `/shared/results/output_task_123.json`). HakuRiver writes `stdout`/`stderr` to `shared_dir/task_outputs` and `shared_dir/task_errors` automatically, but your script should write its *specific* output data files elsewhere in shared storage.
-   **Scripts and Executables**: Place your scripts or custom executables in `shared_dir/shared_data/` (or another mounted location). Make them executable (`chmod +x`). Reference them using their container path (e.g., `/shared/my_script.sh`).
-   **Working Directory**: By default, Docker tasks might have their working directory set to `/app` or `/`, depending on the image. You can explicitly set the working directory in your `hakuriver.task submit` command using the `--workdir` flag (passed to `docker run`), often setting it to `/shared` to easily access shared data.

## Using Local Temporary Storage

The `local_temp_dir` on each Runner is mounted into Docker containers as `/local_temp` and its path is often exposed via an environment variable (`HAKURIVER_LOCAL_TEMP_DIR`) for Systemd tasks.

-   **Temporary Files**: Use `/local_temp` for creating temporary files, intermediate outputs, or scratch space during task execution. This directory is node-local, often on faster storage (SSD/NVMe), and is **not** shared between nodes or between different tasks on the same node (unless configured explicitly). Data placed here is ephemeral and generally not persisted after the task finishes.
-   **Avoid Shared Temp**: Do not use `/tmp` inside the container for temporary files if you need them to persist briefly or be accessible outside the container during the task's life.

## Container vs. Systemd Fallback (`--container NULL`)

Choose the execution method based on your task's needs:

-   **Docker (Default or `--container NAME`)**:
    -   **Use when:** Reproducible environment is crucial, task dependencies are complex or conflict with the host OS, requires specific libraries not on the host, needs GPU allocation, or for interactive VPS tasks.
    -   **Pros:** Isolated, reproducible, easy environment management via HakuRiver workflow, supports GPU allocation.
    -   **Cons:** Adds container overhead, access to host system requires explicit mounts/privileges.
-   **Systemd Fallback (`--container NULL`)**:
    -   **Use when:** Task requires direct access to the host OS environment/devices (e.g., specific kernel modules, host filesystem beyond shared mounts), dependencies are simple or already present on the host OS, or Docker is not available/suitable for this specific task type. Only for Command tasks.
    -   **Pros:** Less overhead than Docker, direct host OS access.
    -   **Cons:** Environment reproducibility depends entirely on the host OS setup, no GPU allocation, NUMA binding requires `numactl` on the host, requires passwordless `sudo` for the runner user.

## Resource Requests

-   **Cores (`--cores`)**: Request a realistic number of cores. Don't request more cores than your task can actually utilize efficiently.
-   **Memory (`--memory`)**: Set a memory limit based on your task's expected peak memory usage. Setting a limit helps prevent OOM kills from rogue processes and aids scheduling. Monitor memory usage to fine-tune this.
-   **GPUs (`--target node::gpu_ids`)**: Request specific GPUs only if your task is designed to use them and you are using a GPU-ready container. Ensure your script/command inside the container is configured to use the allocated GPUs (e.g., using environment variables like `CUDA_VISIBLE_DEVICES` if needed, although Docker often handles this).

## Scripting Languages

You can write your tasks in any language executable within your chosen environment (container or host OS). Common choices include:

-   **Bash Scripts**: Simple, good for orchestrating command-line tools. Ensure the script is executable (`chmod +x`).
-   **Python Scripts**: Excellent for data processing, ML, scripting. Use `hakuriver.task submit ... python your_script.py`. Ensure Python and required libraries are in the environment.
-   **Compiled Binaries**: Compile your code and place the executable in shared storage. Ensure dependencies (including dynamic libraries) are available in the environment or correctly mounted (HakuRiver helps with binding libraries for Systemd tasks, but for Docker, they should typically be *in* the image).

## Error Handling

-   **Capture Output**: Ensure your script's output goes to stdout/stderr. Avoid writing only to files if you need immediate visibility into progress or errors via HakuRiver's logging.
-   **Meaningful Exit Codes**: Use standard exit codes (0 for success, non-zero for failure). Different non-zero codes can signal different types of errors (e.g., 1 for general error, 2 for bad arguments, etc.).
-   **Error Messages**: Print informative error messages to stderr before exiting with a non-zero code.
-   **Logging Frameworks**: Inside complex scripts (e.g., Python), use standard logging libraries (`logging`) directed to stdout/stderr.

## Parameter Sweeps

For running the same command or script with many different parameter combinations, use the local `hakurun` utility on your client machine. See the [HakuRun Utility Guide](../7. hakurun-utility/1. utility.md) for patterns to integrate `hakurun` with `hakuriver.task submit` (Method 1: many tasks vs. Method 2: one task running `hakurun`).

## Example Script Skeleton

```bash
#!/bin/bash
# my_processing_script.sh
# Example script for a HakuRiver command task

# Exit immediately if a command exits with a non-zero status.
set -e
# Exit if a variable is referenced before being set.
set -u
# Print commands as they are executed (useful for debugging)
# set -x

# --- Configuration ---
# Access arguments passed via hakuriver.task submit
INPUT_FILE="$1"
OUTPUT_DIR="$2"
THREADS="${3:-1}" # Default to 1 thread if not provided

# Access environment variables if needed (set via --env)
# DEBUG_MODE="${DEBUG:-false}" # Example using default value if not set

# Access special HakuRiver environment variables (for Systemd fallback)
# SHARED_DIR="${HAKURIVER_SHARED_DIR:-/shared_fallback_default}"
# LOCAL_TEMP_DIR="${HAKURIVER_LOCAL_TEMP_DIR:-/tmp}"

# Access standard HakuRiver mounts (for Docker)
SHARED_MOUNT="/shared"
LOCAL_TEMP_MOUNT="/local_temp"

# --- Validation (Optional but Recommended) ---
if [ -z "$INPUT_FILE" ]; then
  echo "Error: Input file not specified." >&2
  exit 1
fi

if [ ! -f "$SHARED_MOUNT/$INPUT_FILE" ]; then
  echo "Error: Input file not found in shared storage: $SHARED_MOUNT/$INPUT_FILE" >&2
  exit 1
fi

# --- Task Logic ---
echo "Starting processing for $INPUT_FILE..."
echo "Using $THREADS threads, output to $OUTPUT_DIR"
echo "Running on node: $(hostname)" # Example: get info about the execution node
echo "Using local temp dir: $LOCAL_TEMP_MOUNT"

# Simulate processing work
# Your actual processing command goes here
# Example using a hypothetical processing tool
# /path/to/your_tool --input "$SHARED_MOUNT/$INPUT_FILE" --output "$SHARED_MOUNT/$OUTPUT_DIR/processed_$(basename $INPUT_FILE)" --threads "$THREADS"

# Example: Simple file copy and log
mkdir -p "$SHARED_MOUNT/$OUTPUT_DIR"
cp "$SHARED_MOUNT/$INPUT_FILE" "$SHARED_MOUNT/$OUTPUT_DIR/processed_$(basename $INPUT_FILE)"
echo "Processing complete."

# --- Clean up (Optional) ---
# Remove temporary files created in $LOCAL_TEMP_MOUNT if necessary

# --- Exit Status ---
exit 0 # Signal success
```
Submit this script:
```bash
hakuriver.task submit --target node1 --container my-ubuntu-env --cores 2 -- \
  /shared/my_processing_script.sh input.txt results_batch_1 2
```

## Next Steps

-   Learn about [VPS Task Management](../3. vps-tasks/1. management.md) for interactive sessions.
-   Understand [GPU Allocation](../4. gpu-allocation/1. allocation.md) for accelerating tasks.
-   Use the [Web Dashboard](../5. web-dashboard/1. overview.md) to manage and monitor your tasks visually.