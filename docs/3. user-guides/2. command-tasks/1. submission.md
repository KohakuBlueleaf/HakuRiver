# Command Task Submission Guide

This guide explains how to submit, manage, and monitor standard command-line tasks in HakuRiver using the `hakuriver.task` and `hakuriver.client` CLI tools. These tasks typically represent batch jobs or scripts designed to run and then exit.

For guidance on running interactive VPS sessions, see the [VPS Task Management Guide](../3. vps-tasks/1. management.md).

## The `hakuriver.task submit` Command

The primary tool for submitting command tasks is the `hakuriver.task submit` command:

```bash
hakuriver.task submit [OPTIONS] -- COMMAND [ARGUMENTS...]
```

-   `[OPTIONS]`: Flags that control targeting, resources, environment, etc.
-   `--`: **Required separator**. Everything after `--` is treated as the command and its arguments to be executed on the target(s).
-   `COMMAND [ARGUMENTS...]`: The command and its arguments to run on the target node(s).

## Basic Task Submission

To submit a simple task to a specific node:

```bash
hakuriver.task submit --target node1 -- echo "Hello, HakuRiver Command Task!"
```

This will run the `echo` command on `node1` using the default container environment configured on the Host.

## Specifying Target(s)

You can specify one or more targets for task execution using the `--target` flag. Repeat the flag for multiple targets.

### Target Syntax: `HOST[:NUMA_ID][::GPU_ID1[,GPU_ID2,...]]`

-   `HOST`: The hostname of a registered Runner node. This is the minimum required. HakuRiver will select available resources on that node.
-   `:NUMA_ID` (Optional): An integer specifying a specific NUMA node ID on the target host. If used, the task will attempt to bind to this NUMA node. Primarily effective for tasks run via the Systemd fallback (`--container NULL`). Ignored for Docker tasks by default unless configured otherwise.
-   `::GPU_ID1[,GPU_ID2,...]` (Optional): A comma-separated list of integer GPU IDs to make available to the task. Only applicable and effective for tasks running inside a Docker container (`--container` is not `NULL`).

### Examples:

-   **Single Node:** Run on `node1`, using any available resources:
    ```bash
    hakuriver.task submit --target node1 -- my-script.sh
    ```
-   **Multiple Nodes:** Run the same task on `node1`, `node2`, and `node3`. HakuRiver creates three separate task instances as part of a batch.
    ```bash
    hakuriver.task submit --target node1 --target node2 --target node3 -- hostname
    ```
-   **Target Specific NUMA Node (with Systemd fallback):** Run on NUMA node 0 of `node1` (requires `--container NULL`, `numactl` installed/configured on Runner).
    ```bash
    hakuriver.task submit --target node1:0 --container NULL -- numactl --hardware
    ```
-   **Target Specific GPUs (with Docker):** Run on `node3` using GPUs 0 and 1 (requires GPU-enabled Runner/container).
    ```bash
    hakuriver.task submit --target node3::0,1 --container my-cuda-env -- python train_gpu.py
    ```
-   **Multiple Targets with different requirements:**
    ```bash
    # Run on node1 using default resources
    hakuriver.task submit --target node1 -- echo "Task 1"
    # Run on node2 using 4 cores
    hakuriver.task submit --target node2 --cores 4 -- echo "Task 2"
    # Run on node3 using GPUs 0 and 1
    hakuriver.task submit --target node3::0,1 --container my-gpu-env -- echo "Task 3"
    ```
    *Note: Submitting tasks with different options requires separate `hakuriver.task submit` commands unless you script it.*

## Specifying Container Environments

Select the Docker environment to run your task in using the `--container` flag:

-   `--container NAME`: Use the HakuRiver container environment packaged as `<NAME>-<timestamp>.tar` in shared storage. Corresponds to the name used with `hakuriver.docker create-tar`.
-   If `--container` is omitted, the `default_container_name` from the Host configuration is used.
-   `--container NULL`: Use the Systemd fallback method. The command will be run directly on the Runner's host OS using `sudo systemd-run --scope`. This is not containerized task execution.

Examples:
```bash
# Use a specific container environment named 'my-python-env'
hakuriver.task submit --target node1 --container my-python-env -- python /shared/script.py

# Run directly on the host OS using Systemd fallback (no Docker container)
hakuriver.task submit --target node1 --container NULL -- df -h
```
**Note:** Tasks requiring GPUs (`::gpu_ids` syntax) **must** use a Docker container environment. The `--container NULL` option is not compatible with GPU allocation.

## Resource Allocation

Specify CPU and memory requirements for your tasks per target instance:

-   `--cores N`: Request N CPU cores. Used by the Host for scheduling and passed to the Runner (Docker `--cpus`, Systemd `CPUQuota`). Default is 1.
-   `--memory SIZE`: Set a memory limit per task instance (e.g., `512M`, `4G`, `2048K`). Uses 1000-based units (K, M, G). Optional. Used by the Host for scheduling and passed to the Runner (Docker `--memory`, Systemd `MemoryMax`).
-   `--target node::gpu_ids`: Request specific GPU devices by ID. Used by the Host for scheduling and passed to Docker `--gpus`.

Examples:
```bash
# Request 4 CPU cores on node1
hakuriver.task submit --target node1 --cores 4 -- python /shared/compute_heavy.py

# Request 8GB memory limit on node2
hakuriver.task submit --target node2 --memory 8G -- python /shared/memory_heavy.py

# Request both, plus specific GPUs
hakuriver.task submit --target node3::0,1 --cores 8 --memory 16G --container my-cuda-env -- python /shared/heavy_gpu_task.py
```

## Environment Variables

Pass environment variables to your tasks using the `--env KEY=VALUE` flag. Repeat the flag for multiple variables. These are set in the execution environment (container or systemd scope).

```bash
hakuriver.task submit --target node1 --env MODEL_TYPE=large --env DEBUG=1 -- python /shared/train.py
```

## Additional Mounts

By default, Docker tasks get `/shared` (mapping to `shared_dir/shared_data`) and `/local_temp` (mapping to `local_temp_dir`) mounted. You can add *additional* bind mounts using the `--mount HOST_PATH:CONTAINER_PATH` flag. Repeat for multiple mounts. This overrides any `additional_mounts` set in the Host configuration for this specific task batch.

```bash
hakuriver.task submit --target node1 --mount /data/input:/mnt/input --mount /data/output:/mnt/output:rw -- ls -la /mnt/input
```

## Privileged Mode

To run the Docker container with the `--privileged` flag, use the `--privileged` flag during submission. This overrides the default setting in the Host configuration. **Use with extreme caution** as it grants extensive access to the host system.

```bash
hakuriver.task submit --target node1 --privileged -- docker info
```

## Waiting for Tasks to Complete

By default, `hakuriver.task submit` returns immediately after submitting tasks to the Host. To wait for the submitted task(s) to reach a final state (completed, failed, killed, lost):

```bash
hakuriver.task submit --wait --target node1 -- long_running_command
```
If submitting to multiple targets with `--wait`, the client will wait until *all* tasks in that batch have finished. Use `--poll-interval SEC` to adjust how often the client checks the task status.

## Task Management (`hakuriver.client`)

Once tasks are submitted, use `hakuriver.client` for general management actions based on Task ID.

### Checking Task Status

To check the detailed status of a specific task (Command or VPS):

```bash
hakuriver.client status <task_id>
```
The output includes command, arguments, status, assigned node, timestamps, exit code, error message, required resources (cores, memory, GPUs), target NUMA ID, container used, and more.

### Viewing Task Output (Command Tasks Only)

Standard output and standard error logs for Command tasks are written to files in the shared directory. You can view their content via the Host API:

```bash
# View standard output
hakuriver.task stdout <command_task_id>

# View standard error
hakuriver.task stderr <command_task_id>
```
VPS tasks typically manage their own logging (e.g., sshd logs) and do not use these shared output files.

### Killing Tasks

To request the termination of a running, pending, or assigning task (Command or VPS):

```bash
hakuriver.client kill <task_id>
```
The Host marks the task as `killed` in the DB and signals the assigned Runner to stop the associated process/container.

### Pausing and Resuming Tasks

Tasks running via Docker or Systemd might support pausing and resuming.

```bash
# Pause a running task
hakuriver.client command <task_id> pause

# Resume a paused task
hakuriver.client command <task_id> resume
```
The Host forwards these commands to the assigned Runner, which attempts to pause/unpause the Docker container or send SIGSTOP/SIGCONT signals to the systemd scope process.

## Batch Task Submission with `hakurun`

HakuRiver includes a powerful local utility called `hakurun` for generating command variations for parameter sweeps. You can combine it with `hakuriver.task submit`. See the [HakuRun Utility Guide](../7. hakurun-utility/1. utility.md) for full details on `hakurun` syntax.

### Method 1: Local Parameter Generation, Distributed Execution

Generate commands with `hakurun` locally, then submit *each resulting command* as a separate `hakuriver.task submit` request. This creates multiple independent tasks in the HakuRiver cluster.

```bash
# Example: Run a Python script with combinations of {1,2,3} and {A,B} on node1
# Generate commands with hakurun, then pipe to bash for execution
hakurun echo "hakuriver.task submit --target node1 --container my-env -- python /shared/script.py" span:{1..3} span:[A,B] | bash
```
This would execute 6 separate `hakuriver.task submit` commands. Each resulting task can be tracked, killed, or retried individually, and can run on different nodes (or the same node if specified).

### Method 2: Single Task, Local Parameter Expansion

Submit a single HakuRiver task where the command *is* `hakurun`, and `hakurun` runs the parameter sweep *inside* that single task's environment on the assigned Runner.

```bash
# Run hakurun inside a single HakuRiver task on node1
hakuriver.task submit --target node1 --container my-env -- \
  hakurun --parallel python /shared/script.py span:{1..3} span:[A,B]
```
This creates only *one* HakuRiver task. All 6 combinations are executed by that single task on the single assigned Runner node. `--parallel` for `hakurun` means running combinations concurrently *on that one Runner*. This is simpler to manage in HakuRiver but concentrates the load and logging.

## Examples

### Running a Data Processing Task

```bash
# Process a dataset using a Python script in a specific environment
# Assumes 'python-data-science' container is ready and /shared/process_data.py exists
hakuriver.task submit --target nodeA --cores 4 --memory 8G --container python-data-science -- \
  python /shared/process_data.py --input /shared/input/data.csv --output /shared/results/output.json
```

### Running a Parameter Sweep for ML Model Training (Method 1)

```bash
# Generate and submit tasks for different learning rates and hidden layer sizes on nodeB
# This creates 3 * 2 = 6 separate HakuRiver tasks
hakurun echo "hakuriver.task submit --target nodeB --cores 8 --memory 16G --container ml-env -- python /shared/train_model.py" \
  span:[0.001,0.01,0.1] "span:[64,128]" | bash
```

### Running a Parameter Sweep for ML Model Training (Method 2)

```bash
# Submit a single task to a GPU node that runs the sweep internally
# This creates 1 HakuRiver task, which runs 6 combinations on nodeC using GPUs 0 and 1
hakuriver.task submit --target nodeC::0,1 --cores 8 --memory 16G --container my-gpu-env -- \
  hakurun --parallel python /shared/train_model.py \
  span:[0.001,0.01,0.1] span:[64,128]
```

## Best Practices

-   **Resource Allocation**: Always specify appropriate `--cores`, `--memory`, and `--target node::gpu_ids` for your tasks. This helps the Host schedule effectively and prevents tasks from consuming excessive resources on a node.
-   **Shared Storage for Data**: Place input data, scripts, and intended output locations within the `shared_dir` (accessible via `/shared` in Docker tasks). This simplifies command lines and ensures data is accessible regardless of the assigned Runner.
-   **Argument Separation**: Remember the `--` separator when using `hakuriver.task submit`.
-   **Error Handling**: Check task statuses and examine `stdout`/`stderr` logs if tasks fail. Structure your scripts to write informative messages to standard output and standard error.
-   **Choose `hakurun` Method**: Select Method 1 or Method 2 for `hakurun` integration based on whether you need independent task management/distribution (Method 1) or simpler HakuRiver task management/localized execution (Method 2).

## Next Steps

-   Learn how to manage and connect to [VPS Tasks](../3. vps-tasks/1. management.md).
-   Understand [GPU Allocation](../4. gpu-allocation/1. allocation.md) in more detail.
-   Set up and use the [Web Dashboard](../5. web-dashboard/1. overview.md) for a visual interface.
-   Explore [Monitoring](../6. monitoring/1. monitoring.md) options.